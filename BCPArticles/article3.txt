Article 3: By Fernando Ostanelli, Head of Delivery and Felipe Brito, Business Director. In Part II of this article we shared a method on how to determine complexity (or size) of software in a clear, standardized and objective way. In this third and final part we will share our lessons learned throughout our journey. We will also discuss how to use the complexity rule as a stepping stone to manage productivity. Where we are today, We tend to use skepticism in our favor. We used the complexity rule in projects already delivered (of different business sectors) to guarantee that it is suitable to different engagements and that it leads to consistent sizing of stories throughout our portfolio. Several programs of different industries (education, financial services, mining, oil and gas, and pharmaceutical) and different technologies and platforms (Java, Microsoft .NET, Sharepoint, PHP, Drupal, to name a few) have been using the tool consistently and successfully, observing the set of criteria we had first proposed. By the end of June we plan to have 600 people trained. Here are some interesting findings: better alignment across the enterprise on a business-driven estimate process (regardless of the technology or even the business vertical); clarity of rules makes the estimation process very straightforward and easy to replicate throughout the teams, reserving sprint hours to what should be the priority - developing high-quality software; the complexity rule stood still despite being constantly challenged. It showed itself to be robust and adequate. There was no need to include new rules or adapt it because of technology or specific needs; experienced teams acknowledged that without the tool they were not being consistent in terms of velocity (even in the same projects and without change in the teams) nor perceiving all the opportunities of improvement; Here are some relevant outcomes: powerful knowledge base keeping rationale/assumptions when there is not enough detail of the roadmap of future engagements; different interpretations of a specific requirement result in insignificant differences in terms of complexity points; standard deviation of estimates using the complexity rule was 70% smaller than the standard deviation using function points; elimination of suspicion/doubt over estimates of size once everybody interested in the story can estimate it; simplicity to determine scope changes (new stories, change in existing stories) from drill down of scope understanding (e.g.: rules that didn't change but that now are better comprehended). Agilists may defend that it is not important to evaluate/quantify scope change but most of the businesses we have been working with want to understand the amount of complexity points we will be able to deliver in a project (budget is always defined). Ultimately the complexity rule complies with all the requirements we had in mind and allows us to determine how many "Complexity Points" each story has according to business variables and software engineering aspects. With this tool we track size, effort, evolve moving averages and seek productivity improvements. Implementing a Productivity Measurement Program: Our productivity measurement program has a continuous improvement mindset and it encompasses the following stages: 1. Data Gathering: This is the first step. We gather the data related to effort spent during each sprint and the size of software being built. By using our complexity rule, we are able to determine the number of complexity points (the functional complexity) of each story and the overall size of the backlog and of the entire project. It is important to say that complexity points are used here according to the definition used in the complexity rule described previously, where we devise a structured and standardized way to associate complexity points to each story. It is also important to mention that we kept using the well established agile techniques and standard tools (burndown charts, velocity and cycle time management...). The added benefit is the standardized estimates and the possibility to create analysis, comparisons and insights for future improvement opportunities. The effort (in hours) to execute each task of the projects is also registered. We track different disciplines (development, testing, bug correction, block resolution etc). We also register quality information (number of bugs during development, UAT and production). 2. Analysis: There is no just-because gathering of data. With data of multiple sprints and projects we can identify trends, correlations and opportunities. The analysis of a specific engagement leads us to a deeper understanding of the current conditions and give us insights on how to implement improvements towards the business objectives. It also helps us in identifying root causes so we can be more effective with corrective actions. 3. Improvement Action: With the insights we plan specific action. This might be a deeper analysis on how a team is achieving better-than-average results sprint after sprint. It might mean working with top performers to propagate learning and best practices or providing better technical support to a team that is struggling with a challenge. The goal is to identify innovation or adjustment opportunities that can be leveraged consistently within the team (or extended to multiple teams) and establish a higher level of performance. 4. Inspection:And then validate this cycle rapidly! The hypotheses need to be confirmed (or not) in the following sprint so further action is promptly taken and continuous improvement is reinforced. Each improvement added by the team leads to straightforward results. It may be improving quality, organizational learning or overall productivity. The team works smarter not harder. When productivity improves we update moving averages for a given task and we are then able to provide more working software and better business outcomes to the product owner. Measuring Productivity with the Complexity Rule - A Real World Case Study. Project planning and Throughput Commitment: On October 2013 we took over the responsibility for developing a large system for an energy company. During the first weeks the business analyst team and the product owners analyzed the project backlog according to our complexity rule. This provided everybody with a clear vision of the needed throughput to finish the engagement by October 2014 - a critical business milestone that was essential for the continuity of the initiative. According to our projected throughput, we were able to create an accurate plan (sizing, team structure and onboarding and training strategy...) Moreover, the team was able to establish specific hypotheses that needed to be validated and plan for risks that should be mitigated throughout the project. We could be very flexible in terms of what would be delivered - aligned with the business priorities -, but also have a solid commitment in terms of rate of complexity points per sprint. Productivity Improvement Throughout the Project: We had the first 3 months setting up and onboarding 50 people. From day 0, we gathered the number of complexity points delivered per sprint, effort needed to deliver each task, number of bugs, etc. With the set of metrics the team (not only management) was able to understand its performance, compared with previous track records, make root cause analysis, put corrective actions in place and confirm hypothesis of improvement actions they had adopted previously. We had bumps along the way, but the data really helped the team to adjust its course quickly and we are very happy with the trend and the outcomes. Some numbers: Sprint 1 (October): Productivity 10.34 hours/Complexity Point, Sprint 15 (May): Productivity 3.59 h/CP, Q4/13 Productivity: 6.33 h/CP, Q1/14 Productivity: 4.13 h/CP. What's next? Here are some challenges and some goals for the near future: keep building up our corporate metric systems so we can intensify our organizational learning, creating powerful correlations and generating insights according to clients, business verticals, technology and seniority; reinforce the business-driven mindset of the rule: we occasionally see internally people still misinterpreting the rule because they have such a long history of thinking in more technical terms; invite our clients to extend the usage of the rule so they can adopt it as a corporate strategy and are then able to realize similar benefits internally and with other partners; work with the agile community to evolve this model. Discuss challenges that the rule might still not address and improve it. If you would like to talk about it, please send us a message!