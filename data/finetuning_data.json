Total articles: 124
{"prompt": "# Ways to measure continuous testing  You can measure the results of continuous testing in your environment by doing the following:  ", "completion": "Continuous testing"}
{"prompt": "# Case study: Datastore  A tightly coupled architecture can impede everyone's productivity and ability to safely make changes. In contrast, a loosely coupled architecture promotes productivity and safety with well-defined interfaces that enforce how modules connect with each other. A loosely coupled architecture lets small and productive teams make changes that can be deployed safely and independently. And because each service also has a well-defined API, it enables easier testing of services and the creation of contracts and service level agreements (SLAs) between teams.    Randy Shoup describes this architecture as follows:  This kind of service-oriented architecture allows small teams to work on smaller and simpler units of development that each team can deploy independently, quickly, and safely.  ", "completion": "Architecture"}
{"prompt": "# DevOps tech: Continuous testing  The key to building quality into software is getting fast feedback on the impact of changes throughout the software delivery lifecycle. Traditionally, teams relied on manual testing and code inspection to verify systems' correctness. These inspections and tests typically occurred in a separate phase after 'dev complete.' This approach has the following drawbacks:   * Manual regression testing is time-consuming to execute and expensive to perform, which makes it a bottleneck in the process. Software can't be released frequently and developers can't get quick feedback.  * Manual tests and inspections are not reliable, because people are poor at repetitive tasks like manual regression tests, and it is hard to predict the impact of changes on a complex software system through inspection.  * Once software is 'dev complete', developers have to wait a long time to get feedback on their changes. This usually results in substantial work to triage defects and fix them. Performance, security, and reliability problems often require design changes that are even more expensive to address when discovered at this stage.  * Long feedback cycles also make it harder for developers to learn how to build quality code, and under schedule pressure development teams can sometimes treat quality as 'somebody else's problem'.  * When developers aren't responsible for testing their own code it's hard for them to learn how to write testable code.  * For systems that evolve over time, keeping test documentation up to date requires considerable effort. Instead, teams should:   * Perform all types of testing continuously throughout the software delivery lifecycle.  * Create and curate fast, reliable suites of automated tests which are run as part of your continuous delivery pipelines. Not only does this help teams build (and learn how to build) high quality software faster, DORA's research shows that it also drives improved software stability, reduced team burnout, and lower deployment pain.  ", "completion": "Continuous testing"}
{"prompt": "# How to implement continuous testing  To build quality into the software, you must continually run both automated and manual tests throughout the delivery process to validate the functionality and architecture of the system under development. This discipline has both an organizational and a technical component. Organizationally, DORA's research finds that teams do better when they:   * Allow testers to work alongside developers throughout the software development and delivery process. (Note that 'tester' is a role, not necessarily a full-time job, although this is a common pattern discussed below.)  * Perform manual test activities such as exploratory testing, usability testing, and acceptance testing throughout the delivery process. A key technical activity is building and maintaining a set of automated test suites, including:   * Unit tests. These typically test a single method, class, or function in isolation, providing assurance to developers that their code operates as designed. To ensure that the code is testable and tests are maintainable, write your unit tests before writing code, a technique known as test-driven development  (TDD).  * Acceptance tests: These typically test a running app or service (usually with dependencies replaced by test doubles) to provide assurance that a higher level of functionality operates as designed and that regression errors have not been introduced. Example acceptance tests might check for the business acceptance criteria for a user story or the correctness of an API. Write these tests as part of the development process. No one should be able to declare their work 'dev complete' unless automated acceptance tests are passing. The following diagram, initially created by Brian Marick  and later referenced in the book Agile Testing: A Practical Guide for Testers and Agile Teams, shows the types of automated and manual tests to run.    The automated tests highlighted in the preceding diagram fit in a continuous delivery deployment pipeline. In such pipelines, every change runs a build that creates software packages, executes unit tests, and possibly performs other checks, such as static analysis. After these packages pass the first stage, more comprehensive automated acceptance tests, and likely some nonfunctional tests such as performance tests and vulnerability scans, run against automatically deployed running software. Any build that passes the acceptance stage is then typically made available for manual exploration and usability testing. Finally, if no errors are found in these manual steps, the app is considered releasable.  Running tests continuously as part of a pipeline contributes to quick feedback for developers, a short lead time from check-in to release, and a low error rate in production environments. Developers have most of their work validated in a matter of minutes, instead of days or weeks, so they can fix bugs as soon as possible.  The following diagram shows an example of a simple linear deployment pipeline. In this example, green means no problems were found, and red means that one or more problems were discovered.    In the deployment pipeline pattern, every change creates a release candidate and the quick feedback loop helps to catch problems as early in the process as possible. When a package reaches the end of the pipeline and the team still doesn't feel comfortable with releasing it, or if they discover defects in production, the pipeline must be improved, perhaps by adding or updating tests.  ", "completion": "Continuous testing"}
{"prompt": "# Common pitfalls   * Not having developers involved in testing. DORA's research shows that when developers are primarily responsible for creating and maintaining suites of automated tests, and when it is easy for developers to fix acceptance test failures, this drives improved performance. When other groups own the test automation, two problems often arise:  Test suites are frequently in a broken state. Code changes might require tests to be updated. If developers are not responsible for test automation, the build pipeline stays broken until the responsible team fixes the tests. Developers write code that is hard to test. Developers tend to solve the problem they are given without thinking about how it will be tested. This can lead to poorly designed code and expensive, hard-to-maintain test suites.  Testers and QA teams continue to have an important role in this way of working. Testers have a unique perspective on the system because they understand how users interact with it. It's a good practice to pair testers with developers to create and evolve the suites of automated tests, using screen sharing tools if teams are not physically colocated. This way, they can learn from each other and solve problems in real time. Testers also play an essential role performing exploratory testing and usability testing, as well as helping to curate test suites. Not having developers involved in testing. DORA's research shows that when developers are primarily responsible for creating and maintaining suites of automated tests, and when it is easy for developers to fix acceptance test failures, this drives improved performance. When other groups own the test automation, two problems often arise:   * Test suites are frequently in a broken state. Code changes might require tests to be updated. If developers are not responsible for test automation, the build pipeline stays broken until the responsible team fixes the tests.  * Developers write code that is hard to test. Developers tend to solve the problem they are given without thinking about how it will be tested. This can lead to poorly designed code and expensive, hard-to-maintain test suites. Testers and QA teams continue to have an important role in this way of working. Testers have a unique perspective on the system because they understand how users interact with it. It's a good practice to pair testers with developers to create and evolve the suites of automated tests, using screen sharing tools if teams are not physically colocated. This way, they can learn from each other and solve problems in real time. Testers also play an essential role performing exploratory testing and usability testing, as well as helping to curate test suites.   * Failing to curate your test suites. Make sure you continuously review and improve your test suites to better find defects and keep complexity and cost under control. For example:  Acceptance test suites should typically represent real end-to-end  user journeys through the system, rather than just collections of automated acceptance criteria. As your product evolves, so will these scenarios, and the test suites validating them. For more information on this process, see the video Setting a Foundation For Successful Test Automation by Angie Jones. If every time you change your code you must also change multiple unit tests, you're probably over-relying on mocking, or failing to prune your unit test suite. Keep your test suites well-factored. If every change to your UI causes multiple acceptance tests to fail, use the page object pattern  to decouple your tests from the system under test. If your tests are expensive to maintain, this could point to problems with your software's architecture. Make sure you continue to invest in making your software easy to test, including incorporating refactoring  into your team's daily work.  Failing to curate your test suites. Make sure you continuously review and improve your test suites to better find defects and keep complexity and cost under control. For example:   * Acceptance test suites should typically represent real end-to-end  user journeys through the system, rather than just collections of automated acceptance criteria. As your product evolves, so will these scenarios, and the test suites validating them. For more information on this process, see the video Setting a Foundation For Successful Test Automation by Angie Jones.  * If every time you change your code you must also change multiple unit tests, you're probably over-relying on mocking, or failing to prune your unit test suite.  * Keep your test suites well-factored. If every change to your UI causes multiple acceptance tests to fail, use the page object pattern  to decouple your tests from the system under test.  * If your tests are expensive to maintain, this could point to problems with your software's architecture. Make sure you continue to invest in making your software easy to test, including incorporating refactoring  into your team's daily work.  * Having the wrong proportion of unit and acceptance tests. A specific design goal of an automated test suite is to find errors as early as possible. This is why faster-running unit tests run before slower-running acceptance tests, and both are run before any manual testing. You should find errors with the fastest category of test. When you find an error in an acceptance test or during exploratory testing, add a unit test to make sure this error is caught faster, earlier, and cheaper next time. Mike Cohn described the ideal test automation pyramid, shown in the following diagram, where most of the errors are caught using unit testing.  Having the wrong proportion of unit and acceptance tests. A specific design goal of an automated test suite is to find errors as early as possible. This is why faster-running unit tests run before slower-running acceptance tests, and both are run before any manual testing.  You should find errors with the fastest category of test. When you find an error in an acceptance test or during exploratory testing, add a unit test to make sure this error is caught faster, earlier, and cheaper next time. Mike Cohn described the ideal test automation pyramid, shown in the following diagram, where most of the errors are caught using unit testing.     * Tolerating unreliable tests. Tests should be reliable: that is, when the tests pass we should be confident the software is releasable, and test failures should indicate a real defect. In particular, don't tolerate flaky tests. Read about Google's mitigation strategy for flaky tests. Tolerating unreliable tests. Tests should be reliable: that is, when the tests pass we should be confident the software is releasable, and test failures should indicate a real defect. In particular, don't tolerate flaky tests. Read about Google's mitigation strategy for flaky tests.  ", "completion": "Continuous testing"}
{"prompt": "# Ways to improve continuous testing  If your organization doesn't yet have a culture of unit testing by developers, don't worry. Unit testing wasn't a widespread practice at Google in its early years. The current culture of comprehensive unit testing was driven by a group of volunteers at Google called the Testing Grouplet. Read how they helped drive the adoption of unit testing  by building a community of practice focused on propagating testing knowledge throughout Google and persuading developers of the value of unit testing.  If you don't have enough test automation, get started by building a skeleton deployment pipeline. For example, create a single unit test, a single acceptance test, and an automated deployment script that stands up an exploratory testing environment, and thread them together. Then incrementally increase test coverage and extend your deployment pipeline as your product or service evolves.  If you're already working on a brownfield system, follow the guidance in this article, but don't stop to retrofit a comprehensive suite of automated tests. Instead, write a small number of acceptance tests for the high-value functionality. Then, make sure you require developers to write unit and acceptance tests for any new functionality, and any functionality you are changing. Consider using TDD to improve the quality and maintainability of both main and test code, and finally, ensure that when your acceptance tests break, you write unit tests to discover the defect faster in the future.  If you have a test suite that is expensive to maintain and unreliable, don't be afraid to prune it down. A test suite of ten tests that is reliable, fast, and trustworthy is much better than a test suite of hundreds of tests that is hard to maintain and that nobody trusts.  ", "completion": "Continuous testing"}
{"prompt": "# DevOps tech: Deployment automation  Deployment automation is what enables you to deploy your software to testing and production environments with the push of a button. Automation is essential to reduce the risk of production deployments. It's also essential for providing fast feedback on the quality of your software by allowing teams to do comprehensive testing as soon as possible after changes.  An automated deployment process has the following inputs:   * Packages created by the continuous integration (CI) process (these packages should be deployable to any environment, including production).  * Scripts to configure the environment, deploy the packages, and perform a deployment test (sometimes known as a smoke test).  * Environment-specific configuration information. We recommend that you store the scripts and configuration information in version control. Your deployment process should download the packages from an artifact repository (for example, Artifact Registry, Nexus, Artifactory, or your CI tool's built-in repository).  The scripts usually perform the following tasks:   * Prepare the target environment, perhaps by installing and configuring any necessary software, or by starting up a virtual host from a pre-prepared image in a cloud provider such as Google Cloud.  * Deploy the packages.  * Perform any deployment-related tasks such as running database migration scripts.  * Perform any required configuration.  * Perform a deployment test to make sure that any necessary external services are reachable, and that the system is functioning. ", "completion": "Deployment automation"}
{"prompt": "# How to implement deployment automation  When you design your automated deployment process, we recommend that you follow these best practices:   * Use the same deployment process for every environment, including production. This rule helps ensure that you test the deployment process many times before you use it to deploy to production.  * Allow anyone with the necessary credentials to deploy any version of the artifact to any environment on demand in a fully automated fashion. If you have to create a ticket and wait for someone to prepare an environment, you don't have a fully automated deployment process.  * Use the same packages for every environment. This rule means that you should keep environment-specific configuration separate from packages. That way, you know that the packages you are deploying to production are the same ones that you tested.  * Make it possible to recreate the state of any environment from information stored in version control. This rule helps ensure that deployments are repeatable, and that in the event of a disaster recovery scenario, you can restore the state of production in a deterministic way. Ideally, you have a tool that you can use autonomously to make deployments, that records which builds are currently in each environment, and that records the output of the deployment process for audit purposes. Many CI tools have such features.  ", "completion": "Deployment automation"}
{"prompt": "# Common pitfalls in deployment automation  When you automate your deployment process, you face the following pitfalls:   * Complexity of the existing process.  * Dependencies between services.  * Components that are not designed for automation.  * Poor collaboration between teams. The first pitfall is complexity. Automating a complex, fragile manual process produces a complex, fragile automated process. You first need to re-architect for deployability. This means making the deployment script as simple as possible and pushing the complexity into the application code and infrastructure platform. Look for deployment failure modes and ask how you could avoid them by making your services, components, infrastructure platform, and monitoring smarter. Cloud-native applications running on a platform-as-a-service such as App Engine, Cloud Run, or Pivotal Cloud Foundry  can typically be deployed by running a single command, with no deployment scripting required at all: this is the ideal process.  There are two important properties of a reliable deployment process. First, the individual steps of the deployment process should be, to the greatest extent possible, idempotent, so that you can repeat them as many times as needed in the case of a failure. Second, they should be order independent, meaning that components and services should not crash in an uncontrolled way if some other component or service they are expecting is absent. Instead, the services should continue to operate in a degraded fashion until their dependencies become available.  For new products and services, we recommend that you treat these principles as system requirements from the beginning of the design phase. If you are retrofitting automation for an existing system, you might need to do some work either to implement these characteristics or to build in telemetry such that the deployment process can detect inconsistent states and fail gracefully.  The second pitfall is that many deployment processes, particularly in enterprise environments, require orchestration. In other words, you need to deploy multiple services together in a particular order, while you perform other tasks such as database migrations in strict synchronization. Although many enterprise deployment workflow tools exist to help with this situation, these tools are fundamentally band-aids over an architectural problem: tight coupling between the various components and services. Over time, you must address this tight coupling. The goal is that services should be independently deployable, with no orchestration required.  This approach typically requires careful design to ensure that each service supports backward compatibility, such that clients of the service don't require upgrading in lock-step, but can be upgraded independently at a later date. Techniques such as API versioning  can help with this. It's also important to ensure that services can continue to operate (perhaps with some functionality unavailable) even if they are unable to connect to other services that they depend on. This design is good for distributed systems, because it can help prevent cascading failures. Michael Nygard's book 'Release It!' describes a number of patterns to help with designing distributed systems, including circuit breakers. You can even decouple database upgrades from the services they depend on by using the parallel change pattern.  A third common pitfall is components that are not designed for automation. Any deployment process that requires logging into a console and interacting manually by clicking around should be a target for improvement. Today, most platforms (including Google Cloud) offer an API that your deployment script can use. If that's not the case, you need to be creative to avoid such manual intervention, perhaps by finding the tool's underlying configuration file or database and making changes to it directly, or by replacing it with another tool that does have an API.  The last pitfall occurs when developers and IT operations teams aren't in sync. This can happen in a few ways. For example, developers might use one method to deploy and IT operations uses a different one. Or in another example, if the environments are configured differently, you substantially increase the risk of the deployment process being manually performed by IT operations, which introduces inconsistencies and errors. The deployment automation process must be created by developers and IT operations working together. This approach ensures that both teams can understand, maintain, and evolve deployment automation.  ", "completion": "Deployment automation"}
{"prompt": "# Ways to improve deployment automation  The first step is to document the existing deployment process in a common tool that developers and operations have access to, such as Google Docs or a wiki. Then work to incrementally simplify and automate the deployment process. This approach typically includes the following tasks:   * Packaging code in ways suitable for deployment.  * Creating pre-configured virtual machine images or containers.  * Automating the deployment and configuration of middleware.  * Copying packages or files into the production environment.  * Restarting servers, applications, or services.  * Generating configuration files from templates.  * Running automated deployment tests to make sure the system is working and correctly configured.  * Running testing procedures.  * Scripting and automating database migrations. Work to remove manual steps, implement idempotence and order independence wherever possible, and leverage the capabilities of your infrastructure platform wherever possible. Remember: deployment automation should be as simple as possible.  ", "completion": "Deployment automation"}
{"prompt": "# Ways to measure deployment automation  Measuring deployment automation is straightforward.   * Count the number of manual steps in your deployment process. Work to reduce those steps systematically. The number of manual steps increases the deployment time as well as the opportunity for error.  * Measure the level (or percentage) of automation in your deployment pipeline. Work to increase that level continually.  * Determine the time spent on delays in the deployment pipeline. As you work to reduce these delays, understand where and why code stalls in your deployment pipeline. ", "completion": "Deployment automation"}
{"prompt": "# DevOps tech: Trunk-based development  There are two main patterns for developer teams to work together using version control. One is to use feature branches, where either a developer or a group of developers create a branch usually from trunk (also known as main or mainline) and then work in isolation on that branch until the feature they are building is complete. When the team considers the feature ready to go, they merge the feature branch back to trunk.  The second pattern is known as trunk-based development, where each developer divides their own work into small batches  and merges that work into trunk at least once (and potentially several times) a day. The key difference between these approaches is scope. Feature branches typically involve multiple developers and take days or even weeks of work. In contrast, branches in trunk-based development typically last no more than a few hours, with many developers merging their individual changes into trunk frequently.  The following diagram shows a typical trunk-based development timeline:    In trunk-based development, developers push code directly into trunk. Changes made in the release branches—snapshots of the code when it's ready to be released—are usually merged back to trunk (depicted by the downward arrows) as soon as possible. In this approach, there are cases where bug fixes must be cherry picked and merged into releases (depicted by the upward arrow), but these cases are not as frequent as the development of new features in trunk. In cases where releases happen multiple times a day, release branches are not required at all, because changes can be pushed directly into trunk and deployed from there. One key benefit of the trunk-based approach is that it reduces the complexity of merging events and keeps code current by having fewer development lines and by doing small and frequent merges.  In contrast, the following diagram shows a typical non-trunk-based development style:    In this approach, developers make changes to long-lived branches. These changes require bigger and more complex merge events when compared to trunk-based development. This approach also requires additional stabilizing efforts and 'code lock' or 'code freeze' periods to make sure the software stays in a working state, because large merges frequently introduce bugs or regressions. As a result, you must test the post-merge code thoroughly and often have to make bug fixes.  ", "completion": "Trunk-based development"}
{"prompt": "# Ways to measure architectural improvement  Whether on a mainframe or in microservices, facilitating the practices required for architectural improvement is essential for improving software delivery performance (increased deployment frequency with reduced lead time for changes, time to restore service, and change failure rate). As your services and products become less tightly coupled, your deployment frequency should increase. When measuring improvement, consider using deployment rate rather than just count, because deployment count naturally increases as services are added. Lastly, you should see a reduction in time to detect and recover from problems and in the time for changes to reach production.  Aside from taking these deployment and service measures, teams that operate more independently demonstrate improvements in job satisfaction  and team experimentation, and tend to select different technologies and tools based on their needs.  ", "completion": "Architecture"}
{"prompt": "# How to implement trunk-based development  Trunk-based development is a required practice for continuous integration. Continuous integration (CI) is the combination of practicing trunk-based development and maintaining a suite of fast automated tests that run after each commit to trunk to make sure the system is always working.  The point of using continuous integration is to eliminate long integration and stabilization phases by integrating small batches of code frequently. In this way, developers ensure they are communicating what they are doing, and the integration gets rid of big merges that can create substantial work for other developers and for testers.  In the CI paradigm, developers are responsible for keeping the build process green—that is,  up and running. This means that if the CI process fails, developers must stop what they're doing either to fix the problem immediately or to revert the change if it can't be fixed in a few minutes.  Practicing trunk-based development requires in turn that developers understand how to break their work up into small batches. This is a significant change for developers who aren't used to working in this way.  Analysis of DevOps Research and Assessment (DORA) data from 2016  (PDF) and 2017  (PDF) shows that teams achieve higher levels of software delivery and operational performance (delivery speed, stability, and availability) if they follow these practices:   * Have three or fewer active branches in the application's code repository.  * Merge branches to trunk at least once a day.  * Don't have code freezes and don't have integration phases. ", "completion": "Trunk-based development"}
{"prompt": "# Common pitfalls  Some common obstacles to full adoption of trunk-based development include the following:   * An overly heavy code-review process. Many organizations have a heavyweight code review process that requires multiple approvals before changes can be merged into trunk. When code review is laborious and takes hours or days, developers avoid working in small batches and instead batch up many changes. This in turn leads to a downward spiral where reviewers procrastinate with large code reviews due to their complexity. Consequently, merge requests often languish because developers avoid them. Because it is hard to reason about the impact of large changes on a system through inspection, defects are likely to escape the attention of reviewers, and the benefits of trunk-based development are diminished. An overly heavy code-review process. Many organizations have a heavyweight code review process that requires multiple approvals before changes can be merged into trunk. When code review is laborious and takes hours or days, developers avoid working in small batches and instead batch up many changes. This in turn leads to a downward spiral where reviewers procrastinate with large code reviews due to their complexity.  Consequently, merge requests often languish because developers avoid them. Because it is hard to reason about the impact of large changes on a system through inspection, defects are likely to escape the attention of reviewers, and the benefits of trunk-based development are diminished.   * Performing code reviews asynchronously. If your team practices pair programming, then the code has already been reviewed by a second person. If further reviews are required, they should be performed synchronously: when the developer is ready to commit the code, they should ask somebody else on the team to review the code right then. They should not ask for asynchronous review—for example, by submitting a request into a tool and then starting on a new task while waiting for the review. The longer a merge is delayed, the more likely it is to create merge conflicts and associated issues. Implementing synchronous reviews requires the agreement of the team to prioritize reviewing each others' code over other work. Performing code reviews asynchronously. If your team practices pair programming, then the code has already been reviewed by a second person. If further reviews are required, they should be performed synchronously: when the developer is ready to commit the code, they should ask somebody else on the team to review the code right then. They should not ask for asynchronous review—for example, by submitting a request into a tool and then starting on a new task while waiting for the review. The longer a merge is delayed, the more likely it is to create merge conflicts and associated issues. Implementing synchronous reviews requires the agreement of the team to prioritize reviewing each others' code over other work.   * Not running automated tests before committing code. In order to ensure trunk is kept in a working state, it's essential that tests are run against code changes before commit. This can be done on developer workstations, and many tools also provide a facility to run tests remotely against local changes and then commit automatically when they pass. When developers know that they can get their code into trunk without a great deal of ceremony, the result is small code changes that are easy to understand, review, test, and which can be moved into production faster. Not running automated tests before committing code. In order to ensure trunk is kept in a working state, it's essential that tests are run against code changes before commit. This can be done on developer workstations, and many tools also provide a facility to run tests remotely against local changes and then commit automatically when they pass. When developers know that they can get their code into trunk without a great deal of ceremony, the result is small code changes that are easy to understand, review, test, and which can be moved into production faster.  ", "completion": "Trunk-based development"}
{"prompt": "# Ways to improve trunk-based development  Based on the discussion earlier, here are some practices you can implement to improve trunk-based development:   * Develop in small batches. One of the most important enablers of trunk-based development is teams learning how to develop in small batches. This requires training and organizational support for the development team.  * Perform synchronous code review. As discussed previously, moving to synchronous code review, or at least ensuring that developers prioritize code review, helps to ensure that changes don't have to wait hours, or even days, to get merged into trunk.  * Implement comprehensive automated testing. Make sure that you have a comprehensive and meaningful suite of automated unit tests. and that these are run before every commit. For example, if you're using GitHub, you can protect branches  to only allow pull request merges when all tests have passed. The Running builds with GitHub Checks  tutorial shows  how to integrate GitHub Checks  with  Cloud Build.  * Have a fast build. The build and test process should execute in a few minutes. If this seems hard to achieve, it probably indicates opportunities for improvement in the architecture  of the system.  * Create a core group of advocates and mentors. Trunk-based development is a substantial change for many developers, and you should expect some resistance. Many developers simply can't imagine working in this way. A good practice is to find developers who have worked in this way, and have them coach other developers. It's also important to shift some teams over to work in a trunk-based style. One way to do this is to get a critical mass of developers who are experienced with trunk-based development together so that at least one team is following trunk-based development practices. You can then shift other teams over to this style when you feel confident that the team following this practice is performing as expected. ", "completion": "Trunk-based development"}
{"prompt": "# Ways to measure trunk-based development  You can measure the effectiveness of trunk-based development by doing the following.  ", "completion": "Trunk-based development"}
{"prompt": "# DevOps tech: Shifting left on security  Security is everyone's responsibility. The 2016 State of DevOps Report  (PDF) research shows that high-performing teams spend 50 percent less time remediating security issues than low-performing teams. By better integrating information security (InfoSec) objectives into daily work, teams can achieve higher levels of software delivery performance and build more secure systems. This idea is also known as shifting left, because concerns, including security concerns, are addressed earlier in the software development lifecycle (that is, left in a left-to-right schedule diagram).  In software development, there are at least these four activities: design, develop, test, and release. In a traditional software development cycle, testing (including security testing), happens after development is complete. This typically means that a team discovers significant problems, including architectural flaws, that are expensive to fix.  After defects are discovered, developers must then find the contributing factors and how to fix them. In complex production systems, it's not usually a single cause; instead, it's often a series of factors that interact to cause a defect. Defects involving security, performance, and availability are expensive and time-consuming to remedy; they often require architectural changes. The time required to find the defect, develop a solution, and fully test the fix are unpredictable. This can further push out delivery dates.  Continuous delivery borrows from lean thinking the concept of building quality into the product throughout the process. As W. Edwards Deming says in his Fourteen Points for the Transformation of Management, 'Cease dependence on inspection to achieve quality. Eliminate the need for inspection on a mass basis by building quality into the product in the first place.' In this model, rather than taking a purely phased approach, developers work with security and testing experts to design and deliver work in small batches  throughout the product lifecycle.  Research from DevOps Research and Assessment (DORA)  (PDF) shows that teams can achieve better outcomes by making security a part of everyone's daily work instead of testing for security concerns at the end of the process. This means integrating security testing and controls into the daily work of development, QA, and operations. Ideally, much of this work can be automated and put into your deployment pipeline. By automating these activities, you can generate evidence on demand to demonstrate that your controls are operating effectively; this information is useful to auditors, assessors, and anyone else working in the value stream.  ", "completion": "Shifting left on security"}
{"prompt": "# DevOps process: Visibility of work in the value stream  Visibility of work represents the extent to which teams have a good understanding of the flow of work from the business all the way through to customers, and whether they have visibility into this flow, including the status of products and features. Visibility of work is part of a wider group of capabilities that represent lean product management; these capabilities include working in small batches, team experimentation, and visibility into customer feedback. These capabilities predict both software delivery performance and organizational performance (which is measured in terms of profitability, market share, and productivity).  ", "completion": "DevOps process: Visibility of work in the value stream"}
{"prompt": "# How to implement work visibility  Teams that are proficient at this capability have the following characteristics:   * The team understands how work moves through the business from idea to customer, including products or features.  * The team has visibility into the flow of this work.  * The flow of work, including its current state, is shown on visual displays or dashboards.  * Information about the flow of product development work across the whole value stream is readily available. Understanding how work moves through the product or feature development value stream is an essential step in improving workflow. A useful technique is value stream mapping  (VSM). You can create a value stream map by gathering stakeholders from every part of the product development value stream: the business line, design, testing, QA, operations, and support. You break the value stream into 5 to 15 process blocks. In each block, you record the activity that's performed, along with the team that performs it, as shown in the following diagram:    Source: Lean Enterprise  (O'Reilly) by Jez Humble, Joanne Molesky, and Barry O'Reilly, 2014  Next, you analyze the state of work within the value stream, gathering the information to determine barriers to flow. In particular, for each process block, you measure the following key metrics:   * Lead time: the time from the point a process accepts a piece of work to the point it hands that work off to the next downstream process.  * Process time: the time it would take to complete a single item of work if the person performing it had all the necessary information and resources to complete it and could work uninterrupted.  * Percent complete and accurate (%C/A): the proportion of times that a process receives something from an upstream process that it can use without requiring rework. You always record the state of the processes as they really are on the day the exercise is performed. Make sure that you determine the actual metrics, not what people would like the metrics to be.  The following diagram shows an example of the final output:    Source: Lean Enterprise  (O'Reilly) by Jez Humble, Joanne Molesky, and Barry O'Reilly, O'Reilly, 2014.  Look for process blocks that produce poor quality work, which then require a lot of downstream rework (reflected in a low %C/A in the downstream process block), and for processes that have long lead times relative to the process time.  It's important to work with stakeholders to create a future-state value stream map that reflects the optimal state of the value stream at some future date (for example, in 6 months to 2 years). Stakeholders should also agree to re-run the exercise on a regular schedule (for example, every 6 months) to review the current state and to review progress.  A detailed discussion of VSM is out of scope for this document; for more information, we recommend Value Stream Mapping: How to Visualize Work and Align Leadership for Organizational Transformation  by Karen Martin and Mike Osterling.  VSM can depict how work moves through the product development value stream from idea to customer. But to get ongoing visibility into the flow of this work, you need a more dynamic view. For software development, you can use a card wall, a storyboard, or a Kanban board like the one shown in the following diagram.    Source: 'Kanban for Ops' board game, Dominica DeGrandis, 2013.  Using visual displays such as this, and creating of WIP limits to manage flow, are detailed in the articles on WIP limits  and visual management.  Finally, the mapping should include information about the responsibilities of each team, along with statistical data on key metrics such as lead time, deploy frequency, and %C/A.  ", "completion": "DevOps process: Visibility of work in the value stream"}
{"prompt": "# How to implement improved security quality  Shifting the security review process 'left' or earlier in the software development lifecycle requires several changes from traditional information security methods, but is not a significant deviation from traditional software development methods on closer inspection.  The InfoSec team should get involved in the design phase for all projects. When a project design begins, a security review can be added as a gating factor for releasing the design to the development stage. This review process might represent a fundamental change in the development process. This change might require developer training. It might also require you to increase the staff of the InfoSec team, and provide organizational support for the change. While including InfoSec might represent a change in your organization, including new stakeholders in design is not a new concept and should be embraced when considering the benefits.  Providing developers with preapproved libraries and tools that include input from the InfoSec team can help standardize developer code. Using standard code makes it easier for the InfoSec team to review the code. Standard code allows automated testing to check that developer are using preapproved libraries. This can also help scale the input and influence from InfoSec, because that team is typically understaffed compared to developers and testers.  Building security tests into the automated testing  process means that code can be continuously tested  at scale without requiring a manual review. Automated testing can identify common security vulnerabilities, and it can be applied uniformly as a part of a continuous integration pipeline or build process. Automated testing does require you to design and develop automated security tests, both initially and as an on-going effort as new security tests are identified. This is another opportunity to scale the input from the InfoSec team.  ", "completion": "Shifting left on security"}
{"prompt": "# Common pitfalls  Some common pitfalls that prevent teams from shifting security left include the following:   * Failing to collaborate with the InfoSec team. The biggest mistake is when teams fail to collaborate with their InfoSec teams. InfoSec is a vitally important function in an era where threats are ubiquitous and ongoing.  * Understaffing InfoSec teams. InfoSec teams are often poorly staffed. James Wickett, Senior Security Engineer at Verica, cites  a ratio of 1 InfoSec person per 10 infrastructure people per 100 developers in large companies.  * Engaging too late with the InfoSec team. In many cases, the InfoSec gets involved only at the end of the software delivery lifecycle, when it is usually painful and expensive to make changes that are necessary to improve security.  * Being unfamiliar with common security risks. Many developers are unaware of common security risks such as the OWASP Top 10  and how to prevent them. ", "completion": "Shifting left on security"}
{"prompt": "# Ways to improve security quality  You can improve software delivery performance and security quality by doing the following:   * Conduct security reviews. Conduct a security review for all major features while ensuring that the security review process doesn't slow down development.  * Build preapproved code.  Have the InfoSec team build preapproved, easy-to-consume libraries, packages, toolchains, and processes for developers and IT operations to use in their work.  * Integrate security review into every phase. Integrate InfoSec into the daily work of the entire software delivery lifecycle. This includes having the InfoSec team provide input during the design of the application, attending software demos, and providing feedback during demos.  * Test for security. Test security requirements as a part of the automated testing process including areas where preapproved code should be used.  * Invite InfoSec to demos. If you include the InfoSec team in your application demos, they can spot security-related weaknesses early, which gives the team ample time to fix. ", "completion": "Shifting left on security"}
{"prompt": "# Ways to measure security quality  Based on the stated ways to improve outlined above, you can measure security in the following ways.  ", "completion": "Shifting left on security"}
{"prompt": "# DevOps tech: Architecture  Research from the DevOps Research and Assessment (DORA) team shows that architecture is an important predictor for achieving continuous delivery. Whether you're using Kubernetes or mainframes, your architecture enables teams to adopt practices that foster higher levels of software delivery performance.  When teams adopt continuous delivery practices, adopting the following architectural practices drives successful outcomes:   * Teams can make large-scale changes to the design of their systems without the permission of somebody outside the team or depending on other teams.  * Teams are able to complete work without needing fine-grained communication and coordination with people outside the team.  * Teams deploy and release their product or service on demand, independently of the services it depends on or of other services that depend on it.  * Teams do most of their testing on demand, without requiring an integrated test environment.  * Teams can deploy during normal business hours with negligible downtime. It's possible to achieve these outcomes with mainframe technologies. It's also possible to fail to achieve them even when using the latest, most trendy technologies. Many organizations invest lots of time and effort in adopting technologies, but fail to achieve critical software delivery outcomes, due to limitations imposed by architecture.  When the architecture of the system is designed to enable teams to test, deploy, and change systems without dependencies on other teams, teams require little communication to get work done. In other words, both the architecture and the teams are loosely coupled.  This connection between communication bandwidth and systems architecture was first discussed by Melvin Conway, who said, 'organizations which design systems ... are constrained to produce designs which are copies of the communication structures of these organizations.' To counteract tightly-coupled architectures and help support better communication patterns, teams and organizations can use the Inverse Conway Maneuver, whereby team structures and patterns are designed to promote the expected architectural state. In this way, team communication patterns support and enforce the architectural patterns that are built.  With a tightly coupled architecture, small changes can result in large-scale, cascading failures. As a result, anyone working in one part of the system must constantly coordinate with anyone else working in another part of the system, including navigating complex and bureaucratic change management processes.  Microservices architectures are supposed to enable these outcomes, as should any true service-oriented architecture. In practice, many so-called service-oriented architectures don't permit testing and deploying services independently of each other, and thus won’t let teams achieve higher software delivery performance. It's essential to be strict about these outcomes when implementing service-oriented and microservice architectures.  ", "completion": "Architecture"}
{"prompt": "# How to implement architectures for continuous delivery  Consider the major architectural archetypes. Randy Shoup, formerly an Engineering Director for App Engine and VP of Engineering at WeWork, observed the following:  Given the pros and cons of architectural archetypes, each fits a different evolutionary need for an organization.   * Simple at first  * Low interprocess latencies  * Single codebase, one deployment unit  * Resource-efficient at small scales  * Coordination overhead increases as team grows  * Poor enforcement of modularity  * Poor scaling  * All-or-nothing deploy (downtime failures)  * Long build times  * Simple at first  * Join queries are easy  * Single schema deployment  * Resource-efficient at small scales  * Tendency for increased coupling over time  * Poor scaling and redundancy (all or nothing, vertical only)  * Difficult to tune properly  * All-or-nothing schema management  * Each unit is simple  * Independent scaling and performance  * Independent testing and deployment  * Can optimally tune performance (caching, replication, etc.)  * Many cooperating units  * Many small repos  * Requires more sophisticated tooling and dependency management  * Network latencies As the table shows, a monolithic architecture that supports a lean product development effort (for example, rapid prototyping of new features, and potential pivots or large changes in strategies) is different from an architecture that needs hundreds of teams of developers, each of whom must be able to independently deliver value to the customer. By allowing the architecture to evolve, you can ensure that your architecture always serves the current needs of the organization. Regardless of the archetype, when architecting to facilitate continuous delivery, teams must be empowered to achieve the capabilities discussed in the introduction to this document.  Building cross-functional teams, with representation from across the organization (product, dev, test, and operations) enables teams to work independently and facilitates building around team boundaries. When your teams are cross-functional, they can function autonomously, experiment with ideas, and choose their own tools. To help with cross-team communication and testing, it can be helpful to have well-defined contracts between services.  Team independence is important, as is the independence of their products and services. Services need to be testable on demand. Adopting techniques around mocking and stubbing  of external services helps reduce the impact of external dependencies and lets teams quickly create test environments. Also, implementing contract testing  of external services helps ensure that dependencies on their service or other services are still met. To truly achieve continuous delivery, an individual team's product or service must be independently acceptance tested and deployed from the services it depends on.  To enable deploy-anytime capabilities, consider implementing blue/green or rolling deployment models, with high degrees of automation. With these models, at least two or more versions of the product or service are running simultaneously. These deployment models allow teams to validate changes and deploy to production with little or no downtime. An important consideration is how data upgrades are performed, meaning data and schema must be done in a backward-compatible manner.  In order to aid the independent deployment of components, we recommend that you create backward-compatible versioned APIs. Ensuring backward compatibility for APIs adds complexity to systems, but the flexibility you gain in terms of ease of deployment pays for the added complexity many times over.  Service-oriented and microservice architectures enable these capabilities because they use bounded contexts  and APIs as a way to decouple large domains into smaller, more loosely coupled units and the use of test doubles and virtualization as a way to test services or components in isolation.  ", "completion": "Architecture"}
{"prompt": "# Common pitfalls in architectures   * Simultaneously releasing many services. In teams where testability and deployability are not prioritized, most testing requires the use of complex and expensive integrated environments. In many cases, deployments require that you simultaneously release many services due to complex interdependencies. These 'big-bang' deployments require teams to orchestrate their work, with many hand-offs and dependencies between hundreds or thousands of tasks. Big-bang deployments typically take many hours or even days, and require scheduling significant downtime. Simultaneously releasing many services. In teams where testability and deployability are not prioritized, most testing requires the use of complex and expensive integrated environments. In many cases, deployments require that you simultaneously release many services due to complex interdependencies. These 'big-bang' deployments require teams to orchestrate their work, with many hand-offs and dependencies between hundreds or thousands of tasks. Big-bang deployments typically take many hours or even days, and require scheduling significant downtime.   * Integrating changes with the changes from hundreds, or even thousands, of other developers. Those developers, in turn, might have dependencies on tens, hundreds, or thousands of interconnected systems. Testing is done in scarce integration test environments, which often require weeks to obtain and configure. These environments are typically not representative of production, reducing the value and accuracy of the testing. The result is not only long lead times for changes (typically measured in weeks or months) but also low developer productivity and poor deployment outcomes. Integrating changes with the changes from hundreds, or even thousands, of other developers. Those developers, in turn, might have dependencies on tens, hundreds, or thousands of interconnected systems. Testing is done in scarce integration test environments, which often require weeks to obtain and configure. These environments are typically not representative of production, reducing the value and accuracy of the testing. The result is not only long lead times for changes (typically measured in weeks or months) but also low developer productivity and poor deployment outcomes.   * Creating bottlenecks in the software delivery process. Example bottlenecks could be a single team that many others rely on either from a manual process standpoint (testing, deployment, and so on) or from a service operation standpoint. In both examples, those bottlenecks create single points of failure and demand that those teams or services scale to meet the demands of the many dependent teams. Creating bottlenecks in the software delivery process. Example bottlenecks could be a single team that many others rely on either from a manual process standpoint (testing, deployment, and so on) or from a service operation standpoint. In both examples, those bottlenecks create single points of failure and demand that those teams or services scale to meet the demands of the many dependent teams.  ", "completion": "Architecture"}
{"prompt": "# Ways to improve your architecture  With an architecture that enables small teams of developers to independently implement, test, and deploy code into production safely and quickly, you can increase developer productivity and improve deployment outcomes. A key feature of service-oriented and microservice architectures is that they're composed of loosely coupled services with bounded contexts. One popular set of patterns for modern web architecture based on these principles is the twelve-factor app.  Randy Shoup observed the following:  In many organizations, services are distinctly hard to test and deploy. Rather than re-architecting everything, we recommend an iterative approach to improving the design of your enterprise system. This approach is known as evolutionary architecture. In this method, it's given that successful products and services will require re-architecting during their lifecycle due to the changing requirements placed on them.  One valuable pattern in this context is the strangler fig application. In this pattern, you iteratively replace a monolithic architecture with a more componentized one by ensuring that new work is done following the principles of a service-oriented architecture. You accept that the new architecture might well delegate to the system it is replacing. Over time, as more and more functionality is performed in the new architecture, the old system is 'strangled.'    Product and service architectures continually evolve. There are many ways to decide what should be a new module or service, and the process is iterative. When deciding whether to make a piece of functionality into a service, consider if it has the following traits:   * Implements a single business function or capability.  * Performs its function with minimal interaction with other services.  * Is built, scaled, and deployed independently from other services.  * Interacts with other services by using lightweight communication methods, for example, a message bus or HTTP endpoints.  * Can be implemented with different tools, programming languages, data stores, and so on. Moving to microservices or a service-oriented architecture also changes many things through the organization as a whole. In his platform rant, Steve Yegge presents several critical lessons learned from moving to a SOA:   * Metrics and monitoring become more important and escalations become more difficult because an issue surfaced in one service could be from a service many service calls away.  * Internal services can produce Denial of Service (DOS) type problems, so quotas and message throttling are important in every service.  * QA and monitoring begin to blend, because monitoring must be comprehensive and must exercise the business logic and data of the service.  * When there are many services, having a service-discovery mechanism becomes important for efficient operation of the system.  * Without a universal standard for running a service in a debuggable environment, debugging issues in other people's services is much harder. ", "completion": "Architecture"}
{"prompt": "# DevOps tech: Empowering teams to choose tools  If you want to achieve higher software delivery performance and increase the job satisfaction  of your technical staff, you should empower them to make informed choices about the tools and technologies they use to do their work. Research  (PDF) from the DevOps Research and Assessment (DORA) team shows this contributes to better continuous delivery and higher software delivery performance. Teams that can choose their own tools are able to make these choices based on how they work and the tasks they need to perform. No one knows better than practitioners what they need to be effective, so it's not surprising that practitioner tool choice helps to drive better outcomes.  Allowing teams to choose tools doesn't mean each team is given free rein to select any tool they want. Introducing technologies without any constraints can increase technical debt and fragility. However, when you combine tool choice with other capabilities—for example, a full view of the system, fast feedback, and the understanding that they are responsible for the code that they write—it helps your technologists make wise decisions about tools they will use and need to support. This pattern has been observed at companies like Google and Netflix, where a preferred technical stack is supported by default. But if a team feels strongly that a different tool or technology is best for their case, they are free to choose it. Teams understand that their choice comes with the understanding that they must also do the work of supporting this new technical stack.  ", "completion": "Empowering teams to choose tools"}
{"prompt": "# How to empower teams to choose tools  When your organization empowers teams to select tools, it's important to balance the freedom to choose tools with the cost to acquire and support them, and the added potential complexity of communicating between teams that use different tools. The following are some ways you might empower teams to choose their own tools.   * Establish a cross-team baseline. With representatives from different teams and cross-functional areas (product managers, developers, testers, operators), establish a baseline of approved tooling. We recommend that the baseline set of tools be large and diverse enough to address the majority of the needs of your organization. Examples of tools to include in the baseline are programming languages and libraries, testing and deployment tools, monitoring infrastructure, and data backends. Establish a cross-team baseline. With representatives from different teams and cross-functional areas (product managers, developers, testers, operators), establish a baseline of approved tooling. We recommend that the baseline set of tools be large and diverse enough to address the majority of the needs of your organization. Examples of tools to include in the baseline are programming languages and libraries, testing and deployment tools, monitoring infrastructure, and data backends.   * Periodically review the tools. Periodically or as a part of sprint retrospectives, critically evaluate the baseline toolset to examine their effectiveness. These reviews also provide opportunities to discuss and demonstrate new technologies. Periodically review the tools. Periodically or as a part of sprint retrospectives, critically evaluate the baseline toolset to examine their effectiveness. These reviews also provide opportunities to discuss and demonstrate new technologies.   * Define a process for exceptions. Create a clearly defined process for deviating from the base toolset. When a new technology is used that's outside of the baseline for a project, document what the new tool is and why it was used. This documentation is critical when troubleshooting and maintaining the project. Additionally, the documentation included in the projects can be used later to justify adding the tool to the baseline. Define a process for exceptions. Create a clearly defined process for deviating from the base toolset. When a new technology is used that's outside of the baseline for a project, document what the new tool is and why it was used. This documentation is critical when troubleshooting and maintaining the project. Additionally, the documentation included in the projects can be used later to justify adding the tool to the baseline.  An alternative approach is to let teams select their tools. With this strategy, each team addresses as much of the software delivery process (business requirements, development, operations) as appropriate, using their own tool chain. However, be sure that you consider the impact on communication between teams and product areas when there are shared resources.  ", "completion": "Empowering teams to choose tools"}
{"prompt": "# Common pitfalls  The most common pitfall when empowering your teams to choose tools is to take an extreme stance—for example, giving your engineers zero choice in the matter or giving your engineers too much choice.  Forcing tools and technologies on practitioners can improve standardization. However, what works in some cases is not necessarily the best solution in every case. This approach also limits opportunities for experimentation and growth, where emerging technologies can be tried and tested. Often, experimenting with new technologies and adopting them results in large performance gains. For example, if no teams had been allowed to experiment with containerization or platform as a service when those were new technologies, their organizations would not have realized the resulting performance gains.  At the other extreme, choosing different tools and technologies for every different project or service can introduce technical debt and increase fragility. Each time something is added to the tool chain, maintenance and operational expenses increase. Over time, those expenses can negate the performance gained from the new technology.  ", "completion": "Empowering teams to choose tools"}
{"prompt": "# Ways to improve tool choice in teams  The key aspect of performance in tool choice is allowing the teams doing the work to select the best tools for the work. Based on that, here are some suggestions:   * Periodically assess the tech stack. During assessments, encourage team members to critically evaluate how well the current tools address requirements. Additionally, during these reviews, discuss issues with the existing tools and potential new tool experimentation can be discussed and planned.  * Proactively investigate new tools for new projects. Have members of the teams think about and experiment with new tools to determine whether those tools are worth supporting. Try implementing a key piece of the new system using both existing and proposed technologies to see whether the expected benefits materialize. When you select technologies, have a good understanding of the costs associated with the technology. These might include licensing, support, and the infrastructure required to run the tools. You might also need to hire more people to help with adopting and maintaining the technology.  * Schedule time to experiment with new tools. Periodically, hold sessions (such as hackathons) where teams can play around with new projects and new technologies. Not all tools will be kept as a result of these experiments. But the important point is that you're easing these new technologies into your stack or decide they aren't appropriate.  * Hold regular presentations to discuss new tools. Sponsor organized meetings (such as lunch meetings) where new tech is presented and discussed. They can be informal meetings where one person does a presentation about a project they are working on in a new tech, or something they are investigating. Informal meetings like these are a good way for the group to talk about new technologies and stay up to date. A good approach is to rotate the presentations, with team members taking turns presenting Or you can invite people from other teams or someone from outside of the company to present. Including people from outside the organization can be particularly helpful, because if they have experience with a tool, they can discuss hidden costs and complexities that will only be apparent after longer-term use. The goal is to find ways to introduce technologies into the conversation and make sure the team is empowered to make the tool and technology decisions that are appropriate for them. An outcome of these conversations may be to stick with the tools they have right now.  ", "completion": "Empowering teams to choose tools"}
{"prompt": "# Ways to measure if teams are empowered to choose tools  The best way to determine whether teams feel that they're empowered to choose tools is to ask. You don't want to measure this by counting how many tools the team uses or how often teams change tools, because the team might be sticking with the same tool, or might be changing tools, because they are being told they have to.  ", "completion": "Empowering teams to choose tools"}
{"prompt": "# DevOps tech: Continuous integration  Software systems are complex, and an apparently simple, self-contained change to a single file can have unintended side effects on the overall system. When a large number of developers work on related systems, coordinating code updates is a hard problem, and changes from different developers can be incompatible.  The practice of continuous integration (CI) was created to address these problems. CI follows the principle  that if something takes a lot of time and energy, you should do it more often, forcing you to make it less painful. By creating rapid feedback loops and ensuring that developers work in small batches, CI enables teams to produce high quality software, to reduce the cost of ongoing software development and maintenance, and to increase the productivity of the teams.  ", "completion": "Continuous integration"}
{"prompt": "# How to implement CI  When your organization practices CI, your developers integrate all their work into the main version of the code base  (known as trunk, main, or mainline) on a regular basis. DevOps Research and Assessment (DORA) research  (PDF) shows that teams perform better when developers merge their work into trunk at least daily. A set of automated tests is run both before and after the merge in order to validate that the changes don't introduce regression bugs. If these automated tests fail, the team stops what they are doing to fix the problem immediately.  CI ensures that the software is always in a working state, and that developer branches don't diverge significantly from trunk. The benefits of CI are significant: research  (PDF) shows that it leads to higher deployment frequency, more stable systems, and higher quality software.  The key elements in successfully implementing continuous integration are:   * Each commit should trigger a build of the software.  * Each commit should trigger a series of automated tests that provide feedback in a few minutes. To implement these elements, you need the following:   * An automated build process. The first step in CI is having an automated script that creates packages that can be deployed to any environment. The packages created by the CI build should be authoritative and used by all downstream processes. These builds should be numbered and repeatable. You should run your build process successfully at least once a day.  * A suite of automated tests. If you don't have any, start by writing a handful of unit and acceptance tests  (PDF) that cover the high-value functionality of your system. Make sure that the tests are reliable. That way, when they fail, you know there's a real problem, and when they pass, you're confident there are no serious problems with the system. Then ensure that all new functionality is covered by tests. Those tests should run quickly, to give developers feedback as soon as possible. Your tests should run successfully at least once a day. Ultimately, if you have performance and acceptance tests, the developers should get feedback from them daily.  * A CI system that runs the build and automated tests on every check-in. The system should also make the status visible to the team. You can have some fun with this—for example, you can use klaxons or traffic lights to indicate when the build is broken. Don't use email notifications; many people ignore email notifications or create a filter that hides notifications. Notifications in a chat system is a better and more popular way of achieving this. Continuous integration, as defined  by Kent Beck and the Extreme Programming community where the term originated, also includes two further practices, which are also predictive of higher software delivery performance:   * The practice of trunk-based development  in which developers work off trunk/mainline in small batches. They merge their work into a shared trunk/mainline at least daily, rather than working on long-lived feature branches.  * An agreement that when the build breaks, fixing it should take priority over any other work. CI requires automated unit tests. These tests should be comprehensive enough to give you confidence that the software works as expected. The tests must also run in a few minutes or less. If the automated unit tests take longer to run, developers won't want to run them frequently. If the tests are run infrequently, then a test failure can originate from many different changes, making it hard to debug. Tests that are run infrequently are hard to maintain.  Creating maintainable suites of automated unit tests is complex. A good way to solve this problem is to practice test-driven development  (TDD), in which developers write automated tests that initially fail, before they implement the code that makes the tests pass. TDD has several benefits, one of which is that it ensures developers write code that's modular and easy to test, which reduces the maintenance cost of the resulting automated test suites. Many organizations don't have maintainable suites of automated unit tests and, despite that, still don't practice TDD.  As described earlier, CI is sometimes considered a controversial practice. CI requires your developers to break up large features and other changes into smaller incremental steps  that can be integrated frequently into trunk. This is a change for developers who aren't used to working in this way. In addition, when teams switch to using small steps, it can take longer to get large features completed. However, in general, you don't want to optimize for the speed at which developers can declare a large feature as completed on a branch. Rather, you want to be able to get changes reviewed, integrated, tested, and deployed as fast as possible. This process results in software development and delivery that is faster and more stable  (PDF) when the changes are small and self-contained, and the branches they live on are short-lived. Working in small batches also ensures that developers get regular feedback on the impact of their work on the system as a whole—from other developers, testers, and customers, as well as from automated performance and security tests. This in turn makes it easier and quicker to detect, triage, and fix any problems.  Despite these objections, helping software development teams implement continuous integration should be the number one priority for any organization wanting to start the journey to continuous delivery.  ", "completion": "Continuous integration"}
{"prompt": "# Common pitfalls  Some common pitfalls that prevent wide adoption of CI include the following:   * Not putting everything into the code repository. Everything that's needed to build and configure the application and the system should be in your repository. This might seem outside of the scope of CI, but it's an important foundation.  * Not automating the build process. Manual steps create opportunities for mistakes and leave steps undocumented.  * Not triggering quick tests on every change. Full end-to-end tests are necessary, but quick tests (usually unit tests) are also important in order to enable fast feedback.  * Not fixing broken builds right away. A key goal of CI is having a stable build from which everyone can develop. If the build can't be fixed in a few minutes, the change that caused the build to break should be identified and reverted.  * Having tests that take too long to run. The tests should not take more than a few minutes to run, with an upper limit of about 10 minutes according to DORA's research  (PDF). If your build takes longer than this, you should improve the efficiency of your tests, add more compute resources so you can run them in parallel, or split out longer-running tests into a separate build using the deployment pipeline pattern.  * Not merging into trunk often enough. Many organizations have automated tests and builds, but don't enforce a daily merge into trunk. This leads to long-lived branches that are much harder to integrate, and to long feedback loops for the developers. ", "completion": "Continuous integration"}
{"prompt": "# Ways to measure CI  The CI concepts discussed earlier outline ways to measure the effectiveness of CI in your systems and development environment, as shown in the following table. Gathering these metrics allows you to optimize your processes and tooling for them. This leads to better CI practices and to shorter feedback loops for your developers.  ", "completion": "Continuous integration"}
{"prompt": "# DevOps tech: Version control  Version control systems like Git, Subversion, and Mercurial provide a logical means to organize files and coordinate their creation, controlled access, updating, and deletion across teams and organizations. Version control is closely related to automation. In fact, automation and continuous integration  rely on these files for the source code of the automation itself, as well as the configuration to be automated and the data to be distributed.  In order to improve software delivery, teams need to use version control for source code, test and deployment scripts, infrastructure and application configuration information, and the many libraries and packages they depend upon. In the version control system, teams must be able to query the current (and historical) state of their environments. Version control also offers direct benefits such as disaster recovery and auditability.  Research shows that comprehensive use of version control, among other capabilities, predicts continuous delivery. In particular, version control helps you meet these critical requirements:   * Reproducibility. Teams must be able to provision any environment in a fully automated fashion, and know that any new environment reproduced from the same configuration is identical. A prerequisite for achieving this goal is having the scripts and configuration information that are required to provision an environment stored in a shared, accessible system. Reproducibility. Teams must be able to provision any environment in a fully automated fashion, and know that any new environment reproduced from the same configuration is identical. A prerequisite for achieving this goal is having the scripts and configuration information that are required to provision an environment stored in a shared, accessible system.   * Traceability. Teams should be able to pick any environment and determine quickly and precisely the versions of every dependency used to create that environment. They should also be able to compare two versions of an environment and see what has changed between them. Traceability. Teams should be able to pick any environment and determine quickly and precisely the versions of every dependency used to create that environment. They should also be able to compare two versions of an environment and see what has changed between them.  These capabilities give teams several important benefits:   * Disaster recovery. When something goes wrong with an environment—for example, a hardware failure or a security breach—teams need to be able to reproduce that environment in a deterministic amount of time in order to be able to restore service. Disaster recovery. When something goes wrong with an environment—for example, a hardware failure or a security breach—teams need to be able to reproduce that environment in a deterministic amount of time in order to be able to restore service.   * Auditability. To demonstrate the integrity of the delivery process, teams must be able to show the path backward from every deployment to the elements it came from, including their version. You enable this through comprehensive configuration management combined with deployment pipelines. Auditability. To demonstrate the integrity of the delivery process, teams must be able to show the path backward from every deployment to the elements it came from, including their version. You enable this through comprehensive configuration management combined with deployment pipelines.   * Higher quality. The software delivery process is often subject to long delays waiting for development, testing, and production environments to be prepared. When this preparation can be done automatically from version control, teams can get feedback on the impact of their changes more rapidly, enabling teams to build quality into their software. Higher quality. The software delivery process is often subject to long delays waiting for development, testing, and production environments to be prepared. When this preparation can be done automatically from version control, teams can get feedback on the impact of their changes more rapidly, enabling teams to build quality into their software.   * Capacity management. When teams want to add more capacity to their environments, the ability to create reproductions of existing servers is essential. This capability enables the horizontal scaling of modern cloud-based distributed systems. Capacity management. When teams want to add more capacity to their environments, the ability to create reproductions of existing servers is essential. This capability enables the horizontal scaling of modern cloud-based distributed systems.   * Response to defects. When teams discover a critical defect, or a vulnerability in some component of their system, they need to release a new version of their software as quickly as possible. Storing all artifacts in version control means teams can roll back to a previously verified working state quickly and reliably. Response to defects. When teams discover a critical defect, or a vulnerability in some component of their system, they need to release a new version of their software as quickly as possible. Storing all artifacts in version control means teams can roll back to a previously verified working state quickly and reliably.  As environments become more complex and heterogeneous, it's progressively harder to achieve these goals. It's impossible to achieve perfect reproducibility and traceability for a complex enterprise system (at a minimum, every real system has state). Thus, a key part of configuration management is working to simplify the architecture, environments, and processes to reduce the investment required to achieve the expected benefits.  ", "completion": "Version control"}
{"prompt": "# How to implement version control  When implementing version control, we recommend that you start by defining in measurable terms the goals you want to achieve. This allows you and your teams to determine the best path to reach those goals. This approach also lets you change direction or reassess those goals if the path you choose is too expensive or takes too long.  A version control system records changes to files stored in the system. These files can be source code, assets, or other documents that might be part of a software development project. Teams make changes in groups called commits or revisions. Each revision, along with metadata related to the revision (such as who made the change and when), is stored in the system. This allows teams to commit, compare, merge, and restore to previous revisions. It also minimizes risks by establishing a way to revert objects in production to previous versions.  Teams must be able to restore production services repeatedly and predictably (and, ideally, quickly) even when catastrophic events occur, so they must check in the following assets to their shared version control repository:   * All application code and dependencies (for example, libraries and static content)  * Any script used to create database schemas, application reference data, and so on  * All environment creation tools and artifacts described in the previous step (for example, VMware or AMI image building scripts or Chef recipes)  * Any file used to create and compose containers (for example, Docker files and buildpacks)  * All supporting automated tests and any manual test scripts  * Any script that supports code packaging, deployment, database migration, and environment provisioning  * Supporting project artifacts (for example, requirements documentation, deployment procedures, and release notes)  * Container orchestration (for example, Kubernetes configuration, Mesos configuration, and Docker Swarm configuration)  * All cloud configuration files (for example, AWS Cloudformation templates, Cloud Deployment Manager configuration, Microsoft Azure Stack DSC files, OpenStack HEAT, Terraform files, and Pulumi stacks)  * Any other script or configuration information required to create infrastructure that supports multiple services (for example, enterprise service buses, database management systems, DNS zone files, configuration rules for firewalls, and other networking devices) Version control can take many forms, apart from traditional file-based version control systems like Git. Teams might have multiple repositories for different types of objects and services that are versioned, labeled, and tagged alongside their source code. For instance, they might store large virtual machine images, ISO files, compiled binaries, and so forth in artifact repositories such as Nexus or Artifactory. Alternatively, they might put objects in blob stores such as Cloud Storage or Amazon S3, or they might put Docker images into Docker registries. These approaches meet the requirements of reproducibility and traceability, and provide the same benefits.  More than re-creating any previous state of the production environment, teams must also be able to re-create the preproduction and build processes. Consequently, they also need to check into version control everything their build processes rely on, including tools and the environments they depend upon.  ", "completion": "Version control"}
{"prompt": "# Common pitfalls in version control  The most common pitfall in using version control is limited application or use; in other words,  applying version control only to software application code. Best practice requires the ability to reproduce all testing and production environments, including the software deployed on them, in a fully automated fashion by using scripts, source code, and configuration information that's stored in version control.  ", "completion": "Version control"}
{"prompt": "# Ways to improve version control  You can improve version control in many ways. Here a few we recommend:   * Ensure that every commit to version control triggers the automated creation of packages that can be deployed to any environment using only information in version control.  * Make it possible to create production-like test environments on demand using only scripts and configuration information from version control, and to create packages using the automated process described in the previous approach.  * Script testing and production infrastructure so that teams can add capacity or recover from disasters in a fully automated fashion. As you implement a version control system, focus on your constraints. For example, what's the biggest blocker to the fast flow of changes from version control to production? Are your builds too slow? Is it hard to re-create deployable packages? Is it difficult to create production-like test environments? These constraints can make it hard to achieve your goals, and might indicate a problem to work on with your system's architecture.  ", "completion": "Version control"}
{"prompt": "# Ways to measure version control  To measure how effectively your teams are using version control in their systems, try these recommendations:   * Application code. Do you use version control for application code? What percentage of application code do you store in version control? How easily and quickly can a team recover application code from the version control system? Application code. Do you use version control for application code? What percentage of application code do you store in version control? How easily and quickly can a team recover application code from the version control system?   * System configurations. Do you use version control for system configurations? What percentage of system configurations do you store in version control? How easily and quickly can teams reconfigure systems from version control? System configurations. Do you use version control for system configurations? What percentage of system configurations do you store in version control? How easily and quickly can teams reconfigure systems from version control?   * Application configuration. Do you use version control for application configurations? What percentage of application configurations do you store in version control? How easily and quickly can teams reconfigure applications from code in the version control system? Application configuration. Do you use version control for application configurations? What percentage of application configurations do you store in version control? How easily and quickly can teams reconfigure applications from code in the version control system?   * Scripts for automating build and configuration. Do you keep scripts for automating build and configuration in version control? What percentage do you store in version control? How quickly and easily can you reprovision systems by using scripts from version control? Scripts for automating build and configuration. Do you keep scripts for automating build and configuration in version control? What percentage do you store in version control? How quickly and easily can you reprovision systems by using scripts from version control?  These recommendations are just the beginning, but they're essential, so we suggest that you start with them and learn how to do them well. Then review this article and identify additional artifacts that you use in developing and delivering software, and ask similar questions: What percentage of those artifacts are in version control? How quickly and easily can your team deploy new systems or configurations using assets from version control?  ", "completion": "Version control"}
{"prompt": "# DevOps tech: Test data management  Automated testing  is a key component of modern software delivery practices. The ability to execute a comprehensive set of unit, integration, and system tests is essential to verify that your app or service behaves as expected, and can be safely deployed to production. To ensure that your tests are validating realistic scenarios, it's critical to supply the tests with realistic data.  Test data is important because it's required by all kinds of tests throughout your test suite, including manual and automated tests. Good test data lets you validate common or high value user journeys, test for edge cases, reproduce defects, and simulate errors.  However, it's hard to use and manage test data effectively. Over-reliance on data defined outside of test scope can make your tests brittle and increase maintenance costs. Dependencies on external data sources can introduce delays and impact test performance. Copying production data introduces risk because it might contain sensitive information. To address these challenges, you need to manage your test data carefully and strategically.  ", "completion": "Test data management"}
{"prompt": "# How to implement test data management  Analysis done by DevOps Research and Assessment (DORA)  shows that successful teams approach test data management with the following core principles:   * Adequate test data is available to run full automated test suites.  * Test data for automated test suites can be acquired on demand.  * Test data does not limit or constrain the automated tests that teams can run. To improve test data management processes, strive to meet each of these conditions in all of your development teams. These practices can also positively contribute to your overall test automation  and continuous integration  capabilities.  ", "completion": "Test data management"}
{"prompt": "# Common pitfalls in test data management  Managing test data can be hard. The following pitfalls are common in test data management:   * Over-reliance on data in testing. In particular, unit tests should not depend on data or state external to the test.  * Using a full copy of the production database, rather than identifying relevant or important portions.  * Not masking or hashing sensitive data.  * Relying on data that is out of date or no longer relevant. ", "completion": "Test data management"}
{"prompt": "# Ways to improve test data management  The following practices can help you use test data more effectively and efficiently:   * Favor unit tests. Unit tests should be independent of each other and any other part of the system except the code being tested. Unit tests should not depend on external data. As defined by the test automation pyramid, unit tests should make up the majority of your tests. Well-written unit tests that run against a well-designed codebase are much easier to triage and cheaper to maintain than higher-level tests. Increasing the coverage of your unit tests can help minimize your reliance on higher-level tests that consume external data.  Favor unit tests. Unit tests should be independent of each other and any other part of the system except the code being tested. Unit tests should not depend on external data. As defined by the test automation pyramid, unit tests should make up the majority of your tests. Well-written unit tests that run against a well-designed codebase are much easier to triage and cheaper to maintain than higher-level tests. Increasing the coverage of your unit tests can help minimize your reliance on higher-level tests that consume external data.     * Minimize reliance on test data. Test data requires careful and ongoing maintenance. As your APIs and interfaces evolve, you must update or re-create related test data. This process represents a cost that can negatively impact team velocity. Hence, it's good practice to minimize the amount of test data needed to run automated tests. Minimize reliance on test data. Test data requires careful and ongoing maintenance. As your APIs and interfaces evolve, you must update or re-create related test data. This process represents a cost that can negatively impact team velocity. Hence, it's good practice to minimize the amount of test data needed to run automated tests.   * Isolate your test data. Run your tests in well-defined environments with controlled inputs and expected outputs that can be compared to actual outputs. Make sure that data consumed by a particular test is explicitly associated with that test, and isn't modified by other tests or processes. Wherever possible, your tests should create the necessary state themselves as part of setup, using the application's APIs. Isolating your test data is also a prerequisite for tests to run in parallel. Isolate your test data. Run your tests in well-defined environments with controlled inputs and expected outputs that can be compared to actual outputs. Make sure that data consumed by a particular test is explicitly associated with that test, and isn't modified by other tests or processes. Wherever possible, your tests should create the necessary state themselves as part of setup, using the application's APIs. Isolating your test data is also a prerequisite for tests to run in parallel.   * Minimize reliance on test data stored in databases. Maintaining test data stored in databases can be particularly challenging for the following reasons:  Poor test isolation. Databases store data durably; any changes to the data will persist across tests unless explicitly reset. Less reliable test inputs make test isolation more difficult, and can prevent parallelization. Performance impact. Speed of execution is a key requirement for automated tests. Interacting with a database is typically slower and more cumbersome than interacting with locally stored data. Favor in-memory databases where appropriate.  Minimize reliance on test data stored in databases. Maintaining test data stored in databases can be particularly challenging for the following reasons:   * Poor test isolation. Databases store data durably; any changes to the data will persist across tests unless explicitly reset. Less reliable test inputs make test isolation more difficult, and can prevent parallelization.  * Performance impact. Speed of execution is a key requirement for automated tests. Interacting with a database is typically slower and more cumbersome than interacting with locally stored data. Favor in-memory databases where appropriate.  * Make test data readily available. Running tests against a copy of a full production database introduces risk. It can be challenging and slow to get the data refreshed. As a result, the data can become out of date. Production data can also contain sensitive information. Instead, identify relevant sections of data that the tests require. Export these sections regularly and make them easily available to tests. Make test data readily available. Running tests against a copy of a full production database introduces risk. It can be challenging and slow to get the data refreshed. As a result, the data can become out of date. Production data can also contain sensitive information. Instead, identify relevant sections of data that the tests require. Export these sections regularly and make them easily available to tests.  ", "completion": "Test data management"}
{"prompt": "# How to measure test data management  As your approach to test data management evolves, it's important to measure your progress against the core principles outlined earlier.   * Adequate test data is available to run full automated test suites. You can measure this by tracking how much time developers and testers spend managing and manipulating data for use in test suites. You can also capture this by perceptual measures (that is, surveys) to ask teams if they have adequate data for their work, or if they feel this is a constraint for them.  * Test data for automated test suites can be acquired on demand. You can measure this as the percentage of key data sets that are available, how often those data sets are accessed, and how often they are refreshed.  * Test data doesn't limit or constrain the automated tests that teams can run. You can measure this as the number of automated tests that can be run without the need to acquire additional test data. You can also capture this with perceptual measures (that is, surveys), to ask teams if they feel that test data limits their automated testing activities. ", "completion": "Test data management"}
{"prompt": "# DevOps tech: Database change management  Database changes are often a major source of risk and delay when performing deployments. DevOps Research and Assessment (DORA) investigated which database-related practices help during the process of implementing continuous delivery, improving both software delivery performance and availability.  DORA's research found that integrating database work into the software delivery process positively contributes to continuous delivery. But how can your teams improve your database delivery as part of implementing continuous delivery? A few practices predict performance outcomes.  DORA discovered that good communication and comprehensive configuration management that includes the database matter. Teams that do well at continuous delivery store their database changes as scripts in version control and manage these changes in the same way they manage production application changes. Furthermore, when changes to the application require database changes, these teams discuss them with the people responsible for the production database, and ensure the engineering team has visibility into the progress of pending database changes.  When teams follow these practices, database changes don't slow them down or cause problems when they perform code deployments.  ", "completion": "Database change management"}
{"prompt": "# How to implement database change management  There are two aspects to implementing effective database change management: cultural and technical. This section discusses both.  Research shows that teams do best when they discuss changes with the people responsible for managing the production database, and when everyone has visibility into the progress of pending database changes.  Discussing proposed changes with production database administrators (DBAs) is important for a few reasons. First, these experts can advise on how best to achieve results, and point out potential issues such as performance problems. (Many operations have very different performance characteristics in production systems when compared to developer workstations). This discussion also gives DBAs insight into what is happening upstream, which helps them better prepare for the impact of upcoming changes.  Making sure everybody has visibility into the progress of changes is also crucial so that teams, including DBAs, can understand which changes are coming up, their testing status, and which schema changes have made it to the various production and non-production shared databases. You can facilitate visibility by:   * Keeping all database schema changes in version control, together with the application code the schema belongs to;  * Using a tool that records which changes have been run against which environments, and what the results were. These practices also ensure that there is a canonical source of truth for all changes, and makes the history of changes easy to access for auditing purposes.  A widely used pattern for versioning database changes is to capture every change as a migration script which is kept in version control, as shown in the following diagram. Each migration script has a unique sequence number, so that you know in which order to apply migrations.    You then ensure that every database instance has a table that records which migrations have been run against that particular instance. In this way you version-control the database schema, so you can use a tool to apply the migration scripts to take the database to the schema version you want. Examples of tools include:   * migrate  (Go)  * alembic  (Python)  * Active Record Migrations  (Ruby on Rails)  * dbup  (.NET)  * Entity Framework Migrations  (.NET)  * Laravel Migrations  (PHP)  * Flyway  (platform-independent)  * Liquibase  (platform-independent) You can also use migrations to create empty database schemas for development and testing.  As shown in the following digram, every database instance has a table that records which migrations you have run against that instance. Then you can perform updates automatically using a tool or script which executes migrations that have not already been applied against the database instance, updating the migrations table after each one successfully completes.    You can manage database changes in the same way you manage application changes: through an automated process that uses version control as its source of truth.  Many organizations schedule downtime for their services when making database schema changes due to the need to coordinate them with application deployments, or due to database table locking during the execution of such changes. Continuous delivery aims to eliminate downtime for deployments, so here are some strategies to make database schema changes without downtime:   * Use an online schema migration framework such as gh-ost  or pt-online-schema-change. These tools create a 'ghost' copy of each table you want to change, migrate the empty copy, and then incrementally copy data from the original table including any updates that happen during the migration. After this process is complete, they replace the original table with the ghost. Some databases, for example Cloud Spanner, can perform schema updates with zero downtime.  * Decouple database changes and application changes with the parallel change pattern. In this pattern, you never mutate existing database objects. Instead, you add new structures alongside old ones. For example, consider changing a column for 'address' to two columns: address_1 and address_2. Instead of deleting the old column and adding the new one, and rolling out a new version of the application at the same time, you add the new columns but keep the old ones. You can do this before the application deployment occurs. Then the new version of the application can look for the new columns, reading from them if they are present and not null, otherwise reading from the old column. The application can then write to both old and new columns, lazily migrating the data, and also allowing for application rollback without requiring a database rollback. In this way the application deployment is decoupled from the database change, which can typically be made without incurring downtime since it doesn't involve migrating data. We have traded off some additional complexity in the application in order to reduce deployment pain. Alternatively, database triggers can be used to keep data in new and old columns synchronized. Decouple database changes and application changes with the parallel change pattern. In this pattern, you never mutate existing database objects. Instead, you add new structures alongside old ones. For example, consider changing a column for 'address' to two columns: address_1 and address_2.  Instead of deleting the old column and adding the new one, and rolling out a new version of the application at the same time, you add the new columns but keep the old ones. You can do this before the application deployment occurs. Then the new version of the application can look for the new columns, reading from them if they are present and not null, otherwise reading from the old column. The application can then write to both old and new columns, lazily migrating the data, and also allowing for application rollback without requiring a database rollback.  In this way the application deployment is decoupled from the database change, which can typically be made without incurring downtime since it doesn't involve migrating data. We have traded off some additional complexity in the application in order to reduce deployment pain. Alternatively, database triggers can be used to keep data in new and old columns synchronized.   * Design and implement a data partitioning and archiving strategy. A major cause of long migrations is database tables with a large number of rows. Make sure your applications are designed to allow for partitioning and archiving of data to avoid tables growing too large. One example of this would be to create multiple instances of a table for each quarter, for example, instead of a survey_answers table, you might have survey_answers_2020Q1, survey_answers_2020Q2 and so forth. Make sure application design and architecture reviews include validating the application's data partitioning / archiving strategy. Design and implement a data partitioning and archiving strategy. A major cause of long migrations is database tables with a large number of rows. Make sure your applications are designed to allow for partitioning and archiving of data to avoid tables growing too large. One example of this would be to create multiple instances of a table for each quarter, for example, instead of a survey_answers table, you might have survey_answers_2020Q1, survey_answers_2020Q2 and so forth. Make sure application design and architecture reviews include validating the application's data partitioning / archiving strategy.   * Use an event sourcing architecture. In an event sourcing architecture, rather than having the database storing the current state of the application, we have it store changes to its state instead, in the form of a log of events known as commands. So when your customer changes their address, rather than updating a table with the customer details, the application issues an address change command which is stored in the database. This is how database transaction logs and version control work, and is a common pattern in distributed systems. In an event sourced architecture, events can be queued allowing database migrations to happen while events queue up. Events in the queue can then be flushed to the database once the migration is complete. Some databases are able to queue queries while schema migrations run, which can be effective if migrations complete quickly enough. Use an event sourcing architecture. In an event sourcing architecture, rather than having the database storing the current state of the application, we have it store changes to its state instead, in the form of a log of events known as commands. So when your customer changes their address, rather than updating a table with the customer details, the application issues an address change command which is stored in the database. This is how database transaction logs and version control work, and is a common pattern in distributed systems. In an event sourced architecture, events can be queued allowing database migrations to happen while events queue up. Events in the queue can then be flushed to the database once the migration is complete. Some databases are able to queue queries while schema migrations run, which can be effective if migrations complete quickly enough.   * Use a NoSQL solution. Some NoSQL databases, such as Firestore  and Cloud BigTable, don't suffer from the issue of downtime created by schema changes. Document databases like Firestore have an implicit schema, which means that the schema is managed at the application layer rather than the database layer. However there are trade-offs associated with using NoSQL databases: they are not optimal for every application. Use a NoSQL solution. Some NoSQL databases, such as Firestore  and Cloud BigTable, don't suffer from the issue of downtime created by schema changes. Document databases like Firestore have an implicit schema, which means that the schema is managed at the application layer rather than the database layer. However there are trade-offs associated with using NoSQL databases: they are not optimal for every application.  As well as eliminating scheduled downtime, you also want to avoid unscheduled downtime. Make sure you test every schema change against a production-like data set  (with any personal or confidential information scrubbed, of course) to make sure your application behaves the way you expect during and after migration. Some organizations create a scrubbed version of their  production database on a daily schedule to use for this purpose. If you are using a database management system with more than one node in production, make sure you're testing against an instance with at least two nodes to help catch distributed system issues.  ", "completion": "Database change management"}
{"prompt": "# Common pitfalls of implementing database change management  There are a few common pitfalls to be aware of when implementing the key practices described here.  First, many organizations are heavily siloed. Often DBAs work on their own separate team which uses its own process to manage changes. When software delivery teams implement a new process for managing database changes without consulting the DBA team they are likely to face resistance to using their new process to make changes to databases managed by the DBA team. This can substantially reduce the benefits of moving to a new process.  The first step is to get together with them to discuss how to achieve the objectives presented in this article. It is important to get their buy-in to any proposed process and technology changes. The best way to do this is to ask them what problems they are facing, see how the ideas presented in this article can help them address these problems, and offer to help out. Ideally delivery teams and DBAs can find a mutually acceptable solution.  This obstacle is often exacerbated by another pitfall: the common situation where multiple applications share the same database schema. This means that teams working on one application cannot alter the schema without potentially impacting other applications. This requires that a single solution be put in place to manage database changes for all applications sharing the database schema. It is certainly possible, and indeed even more beneficial, to implement a version-controlled, self-service mechanism to deploy database changes in this situation. However, it involves careful planning and rollout.  Finally, implementing both migration-based database change management and zero-downtime deployments can involve significant architectural change. This should be taken into consideration when estimating the effort required to implement these practices.  ", "completion": "Database change management"}
{"prompt": "# How to measure database change management  The goal of an effective database change management system is that database changes don't slow down deployments or cause problems. It's worth measuring the percentage of failed changes in which database changes were a contributing factor, and the extent to which work related to database changes contributes towards overall lead time from version control to release.  If database changes require scheduled downtime, this is also an important consideration. To measure the economic impact of scheduled downtime, consider both the potential lost revenue resulting from downtime and the salary costs of paying people to work out of regular hours in order to perform deployments. Deploying outside of business hours can also contribute to team burnout. These impacts can be used to justify the work required to implement the solutions for zero-downtime deployments discussed in this document.  In terms of measuring the level of automation, consider the proportion of database changes that are made in a push-button way using a fully automated process. The goal should be that 100% of database changes are made in this way.  ", "completion": "Database change management"}
{"prompt": "# DevOps tech: Cloud infrastructure  Many organizations are adopting cloud computing. But there's more to cloud than buying your infrastructure from a cloud provider. The US National Institute of Standards and Technologies (NIST) defines  five essential characteristics of cloud computing:   * On demand self-service: Consumers can provision computing resources as needed, without human interaction from the provider.  * Broad network access: Capabilities can be accessed through diverse platforms such as mobile phones, tablets, laptops, and workstations.  * Resource pooling: Provider resources are pooled in a multi-tenant model, with physical and virtual resources dynamically assigned on-demand. The customer may specify location at a higher level of abstraction such as country, state, or data center.  * Rapid elasticity: Capabilities can be elastically provisioned and released to rapidly scale outward or inward on demand, appearing to be unlimited and able to be appropriated in any quantity at any time.  * Measured service: Cloud systems automatically control, optimize, and report resource use based on the type of service such as storage, processing, bandwidth, and active user accounts. The 2019 State of DevOps Report from DevOps Research and Assessment (DORA) found that only 29% of people who claim to have adopted cloud technologies agreed or strongly agreed that they met all five of the characteristics defined above. In both 2018 and 2019, DORA's research found that elite performers were more than 23 times more likely to have met all five essential cloud characteristics than low performers. This may explain why teams and executives who claim to have adopted cloud computing technologies also express frustration at not reaping the expected results.  Because many organizations still use their traditional datacenter practices and processes to manage their cloud infrastructure, they can't achieve the expected benefits, which according to DORA's research include:   * Improved software delivery performance: faster throughput and higher levels of stability  * Better service availability  * Improved cost visibility: in 2019, we found that respondents who met all essential cloud characteristics are 2.6 times more likely to be able to accurately estimate the cost to operate software. They are also twice as likely to be able to easily identify their most operationally expensive applications. For example, many organizations with cloud infrastructure do not allow developers to self-service their environments on demand: they require them to raise tickets or send emails and wait days for environments to be provisioned or for configuration changes to be made. In this case, although the organization may pay for a cloud service, they don't have a cloud by NIST's definition.  When moving to the cloud, organizations must invest in re-designing the processes and practices that they implemented when they were using traditional data centers. They must adopt cloud-native patterns and practices to achieve the agility, stability, availability, and cost transparency of cloud computing. If the underlying technology is in the cloud but the outcomes for practitioners remain unchanged — days or weeks to obtain test environments, provision new infrastructure, or get configuration changes made — then organizations will not reap the results they want.  ", "completion": "Cloud infrastructure"}
{"prompt": "# How to implement cloud infrastructure  Adopting cloud-native processes and practices to implement NIST's five characteristics involves substantial change by several IT functions, including developers, operations teams, information security, procurement, and finance. The changes require close collaboration between these functions in order to identify and resolve conflicting aims.  For example, many developers expect complete control over production infrastructure when using cloud platforms. Information security professionals oppose this practice, and with good reason — without disciplined change management, cloud infrastructure can turn into a fragile work of art that is hard to manage, vulnerable to external threats, and does not meet regulatory requirements.  However, developers can be empowered to provision the resources they require while meeting these requirements. Many organizations have adopted the infrastructure as code paradigm. (GitOps is an example.) Infrastructure configuration is checked into version control, and developers can provision environments, make configuration changes, and execute deployments through an automated mechanism. The mechanism takes the code  from version control and performs operations through the cloud's API on demand without human intervention. Using version control and automation, all actions and their output are logged to provide complete traceability and auditability of every change to each environment.  Infrastructure-as-code allows you to manage changes effectively, and to apply information security controls. For example, you can implement segregation of duties by requiring all changes to the configuration specified in version control to be approved by someone from a specified group of people  (as is done at Google). However, moving to Infrastructure-as-code requires significant engineering effort and process change, including changing policies for implementing information security controls.  Consider another example. While cloud providers typically only bill for infrastructure while it is in use (meeting NIST's fifth characteristic, measured service), this change from fixed-cost infrastructure to variable cost can have significant implications for both procurement and finance. As described in the 2019 State of DevOps Report:  'Some in finance may say that the cloud has not led to cost savings in the short-run, yet we know that it provides greater information transparency. How can this be? While the cloud provides transparent information about costs to the system owners, users do not pay for these costs unless there is a chargeback model or similar mechanism. This can lead to wildly variable costs that go unchecked, making cloud costs unpredictable. In these scenarios, teams that pay for infrastructure may prefer data centers because they are predictable, even though their visibility disincentivizes system users to build more efficient systems. We suggest organizations better align incentives so that system owners have both visibility to build more efficient systems, and the incentives to do so, by using chargeback or similar mechanisms.'  In addition to considering how infrastructure is managed at both the configuration and finance level, applications must often be re-architected to reap the benefits of increased stability, reliability, and agility that cloud can provide. Re-architecting systems to a cloud-native paradigm is discussed in Google Cloud's white paper, Re-architecting to cloud native: an evolutionary approach to increasing developer productivity at scale.   The most crucial consideration is whether your cloud deployment has actually helped your organization achieve more rapid, reliable releases, and higher levels of availability, velocity, and reliability.  ", "completion": "Cloud infrastructure"}
{"prompt": "# Common pitfalls of implementing cloud infrastructure  The biggest obstacles to meeting the five characteristics defined by NIST are:   * Insufficient alignment and collaboration between the organizational functions that must work together in order to implement them;  * Insufficient investment in technical, process, and organizational change. Many organizations begin with a lift and shift approach to moving applications to the cloud. This requires minimal change to applications and to established processes for managing cloud infrastructure, and can be done relatively quickly. However, it also provides minimal benefits. It's important to plan for moving beyond lift and shift to a cloud-native paradigm for new software, as well as existing strategic systems that will continue to evolve.  Moving to the cloud is a multi-year journey. Like all disruptive changes, it's important to start small and experiment rapidly to discover what works and what doesn't, and then be persistent and disciplined about scaling out learnings and effective patterns and practices through the organization.  There will be many obstacles on this journey to overcome, including:   * Redesigning processes, architecture, and governance to meet regulatory requirements in a cloud-native way  * Designing multi-tenant infrastructure architecture that enables teams to self-service deployments and configuration changes while ensuring logical separation between environments, enabling charge-back, and preventing cloud sprawl and orphaned infrastructure  * Building a product development capability for your cloud infrastructure platform  * Helping procurement transition to infrastructure as a metered service rather than a capital investment  * Helping developers understand how to build cloud-native applications  * Enabling operations to move to modern Site Reliability Engineering  (SRE) practices, including deploying infrastructure-as-code as a replacement for manual ticket-based configuration management  * Planning and executing the integration between cloud native systems and non-cloud-based systems, including mainframes and packaged software/COTS Overcoming these obstacles requires a substantial change program  involving sustained investment and collaboration between people at every level of the organization.  ", "completion": "Cloud infrastructure"}
{"prompt": "# How to measure cloud infrastructure  In order to decide what to measure, start by considering what benefits you hope to gain from implementing cloud infrastructure. Then start measuring the extent to which those benefits are being realized.  For example, if your goal is to improve cost visibility, you might track how well you're doing at making sure the cost of infrastructure is correctly billed to the relevant business line, team, product, or environment.  You can also directly measure whether or not you have done a good job of implementing NIST's essential characteristics: ask your teams about the extent to which they agree with the statements of these characteristics listed at the top of this document. Teams that agree or strongly agree are doing well; teams that are neutral or disagree need help and support to remove obstacles to meeting these outcomes. Then consider how many teams that say they are using cloud can actually meet NIST's criteria.  Some aspects of the essential characteristics can also be measured directly by instrumenting your processes. For example, if you have a process for managing infrastructure changes you could measure the time it takes end-to-end to make a change. You can also look at how prevalent cloud-native technologies are in your organization: for example, the proportion of clusters or machines managed using Kubernetes or autoscaling groups rather than manual provisioning, or the time-to-live for hosts. In data centers, long uptimes generally indicate high reliability; in the cloud-native paradigm, configuration changes are often made by starting new virtual hosts with the new configuration rather than making changes to existing hosts. This practice is known as ephemeral infrastructure, in which a long uptime is an indicator of an unpatched machine.  ", "completion": "Cloud infrastructure"}
{"prompt": "# DevOps tech: Code maintainability  It takes a lot of code  to run the systems we build: The Android operating system runs on 12 to 15 million lines of code, Google's monolithic code repository contains over 1 billion lines of code, and a typical smartphone app has 50,000 lines of code.  The 2019 State of DevOps Report from DevOps Research and Assessment (DORA) research shows that the ability of teams to maintain their code effectively is one of a number of technical practices that contribute positively to success with continuous delivery.  If your team is doing a good job with code maintainability, the following are true:   * It's easy for the team to find examples in the codebase, reuse other people's code, and change code maintained by other teams if necessary.  * It's easy for the team to add new dependencies to their project, and to migrate to a new version of a dependency.  * The team's dependencies are stable and rarely break the code. These findings highlight the importance of making it easy for developers to find, reuse, and change code across the whole organization's codebase, and also of implementing practices and tools to help with dependency management.  Code maintainability is a capability that requires organization-wide coordination, since it relies on being able to search, reuse, and change other teams' code. Managing dependencies effectively is often a major source of pain when working with large codebases and large organizations. Tooling that can help avoid problems with dependencies or illuminate the consequences of code changes can improve design decisions and code quality for all engineers, which in turn enables them to work faster and create more stable, reliable software.  ", "completion": "Code maintainability"}
{"prompt": "# How to implement code maintainability  In terms of implementation, it's worth dealing with source code management and dependency management separately, although it's possible for a solution (such as the monolithic repository or monorepo pattern used by Google) to address both concerns.  First, source code management. Enabling everybody to easily find, reuse, and propose changes to any part of the codebase provides:   * Faster delivery: In order to deliver software quickly, teams need to be able to see and propose changes to each other's code. Making this as easy as possible helps transfer knowledge across the organization, and helps unblock teams who need to make changes to other parts of the codebase in order to get their work done.  * Higher levels of stability and availability: In the event of an incident, it's essential to be able to rapidly find and propose changes to any part of the codebase.  * Higher code quality: Refactoring code to improve its internal quality often involves making changes to multiple parts of the codebase. If this is hard, it reduces the likelihood that people will perform refactorings, and increases the cost of doing so. Some organizations, including Google, run cross-team code maintenance projects where individuals go through the codebase fixing maintenance-level items, which relies on the ability to easily access and change code across the organization. Being able to find examples and reuse other people's code depends on being able to easily access and search the entire organization's source code. The simplest way to implement this requirement is to use a single version control platform for the whole organization's code — even if that code is split between multiple repositories within the platform. The more separate version control platforms are in use, the harder it is to find code.  Some organizations will want to keep parts of the codebase locked down so that only people with a need to know can view that part of the codebase (Google is one example of such an organization). Ideally this should be the exception rather than the rule, and software should be architected to minimize the surface area of confidential source code. In many cases, logical segregation of confidential code using the version control system's access control mechanism is sufficient.  It should also be possible to change code maintained by other teams. Typically such changes will require approval from the team that is responsible for maintaining the code in question. Mechanisms such as pull requests, where a branch is created in version control and approval results in merging of the branch, can minimize the friction of allowing other teams to propose changes, while also preventing unauthorized changes, and enforcing information security controls such as segregation of duties.  Next, consider dependencies. Making it easy for teams to add and update dependencies, and ensuring they are stable and rarely break code, means:   * Better security: As dependencies age, it is more likely that vulnerabilities will be discovered in them. It is essential that dependencies are kept up-to-date, particularly after vulnerabilities are found and patched.  * Faster delivery: Using libraries developed by other teams or organizations means you don't have to write your own code to do that job. When you have mechanisms in place to ensure dependencies are stable and rarely break code, you can spend more time on coding and less time on maintenance. Dependency management is a common pain point for software development teams. Keeping dependencies up-to-date and consistent across applications is complex, time-consuming and expensive. Many organizations fail to allocate adequate resources to this task, a problem that is exacerbated by inadequate processes and tooling. This can represent a significant security risk when vulnerabilities are inevitably discovered in dependencies, and the applications using them must be updated.  It is essential to adopt and evolve processes and tooling that make it easy for teams to consume known-good versions of dependencies and upgrade them rapidly, including automated continuous integration  (CI) and testing  to discover if new versions of dependencies contain breaking changes), and to quickly and simply correlate the versions of dependencies in use with the systems that use them.  There are two commonly used models for including dependencies in your software: vendoring and declarative manifests. In vendoring, either the source code or the binary of every dependency is checked into version control along with the application. Alternatively, most modern platforms have a dependency management tool that manages dependencies specified in declarative manifest files checked into version control (for example, Python's pip, Node.js's npm, R's CRAN, and .NET's NuGet).  Whether you vendor or use manifests, the most important considerations are:   * Traceability: Make sure you can trace back from any given package or deployment to the exact version of every dependency used to build it. Without this, it is impossible to debug problems caused by changes to dependencies.  * Reproducibility: Your build process should be as deterministic as possible. Trying to debug problems caused by a build process that behaves differently on different machines is extremely painful. Google implements code maintainability through an approach that is relatively unusual. While our approach has trade-offs and is not for everyone, it does an effective job of enabling teams to meet the objectives described in this article.  Ninety-five percent of Google's software developers worldwide work on a shared, monolithic codebase maintained through a centralized source control system, using a trunk-based development  model. In 2016, the Google codebase included 'approximately one billion files and [had] a history of approximately 35 million commits spanning Google's entire 18-year existence. The repository contains 86 TB of data, including approximately two billion lines of code in nine million unique source files.'  Because all Google's code is kept in a single repository, it's easy to find and change other teams' code. Google provides an internal search engine that allows the entire codebase to be easily searched. Developers have a variety of tools that enable them to create changelists for review and approval. Tests are run against every changelist, and changelists can be updated and commented upon. Google's code review tooling, similar to many other platforms, can suggest reviewers for any given change. Each directory in the Google repository has an OWNERS file that lists the people or groups who can approve changes to files in that directory (similar functionality is available in GitHub, GitLab, and Bitbucket). Fast search, approver suggestions, and automated tests make proposing changes, review, and collaboration both straightforward and robust.  Using these tools, large-scale refactorings that change multiple parts of the codebase are relatively easy to perform and can be done atomically. Google has built tooling that further simplifies and automates the process of making changes that impact significant sections of the codebase. Google tooling includes a build system called Blaze that is used to build and test any changes, including dependencies, from source. (Parts of Blaze were released in the form of the open source tool Bazel.) It's easy for anyone in Google to discover and propose changes to any part of Google, including its infrastructure configuration which is also kept in the same monolithic repository. Code sharing and reuse is straightforward.  Google also has controls in place to manage dependencies on open source software. First, all open source software used at Google must have its source checked in to Google's monolithic repository. Second, only one version of a given library can be checked in to source control at any time. Third, all software is statically linked and built from source. Finally, any time there is a change in the source code of a library, it triggers rebuilds and automated tests for all software which consumes that dependency.  These controls, combined with Google's powerful CI infrastructure, makes it easy to keep our production systems up-to-date with new versions of dependencies. They also help ensure that all systems use consistent versions of a given library (removing the possibility of 'diamond dependency' hell whereby a product relies on two components that each, in turn, rely on different versions of a common library, making the product impossible to build.)  The trade-off from Google's approach to managing dependencies on external code is that it becomes harder to add new dependencies (one of the key outcomes of code maintainability). Any new dependency must have its source code checked into Google's monolithic repository, which means the code must be reviewed and tested both initially and as part of any upgrades. However, this level of rigor helps prevent code with security vulnerabilities making it into Google's products, and ensures that all dependencies have clearly designated maintainers.  ", "completion": "Code maintainability"}
{"prompt": "# Common pitfalls of implementing code maintainability  The main obstacles to making all code universally searchable and changeable by anyone are tool support and organizational culture.  The first common pitfall is multiple version control repositories, or version control repositories that have restrictive access settings. Organizations should ideally have a single version control platform in which all their code is held. The default access should ideally allow anybody in the organization to view any source file, with the possibility to restrict access for sensitive files. There should also be a way to search version control.  In contrast, organizations typically restrict who can make changes to version control. This leads to the second pitfall: a lack of tooling and process for people to make changes to parts of the codebase to which they do not have write access. This can be a significant obstacle both to fixing problems caused — often inadvertently — by other teams whose code you depend on. It also inhibits refactorings that touch multiple parts of the codebase.  In order to mitigate this problem, some modern version control tools provide a way to submit, review, approve, and audit change requests for parts of the codebase to which users don't have write access.  Even given tooling support, organizations need to be comfortable with making codebases available and searchable within the organization and potentially for vendors and contractors. This may not be possible for organizations whose version control repositories contain significant amounts of confidential information and code that cannot be shared between teams.  Using binary dependencies is much more common, but each language has its own toolchain for managing binary dependencies. Creating a standard strategy and conventions that allow dependencies to be effectively managed and tracked across these toolchains across the organization's entire software portfolio is a complex undertaking. Significant investment in CI infrastructure that allows new versions of libraries to be easily tested for compatibility with existing systems — and for vulnerabilities — is necessary to make the process of upgrading third-party libraries on a regular basis tractable.  In practice, most organizations leave it up to teams to manage their dependencies, with highly variable results. As a result it is usually extremely painful to respond rapidly and predictably in the event of a vulnerability being discovered in a library: even finding the services affected is typically a substantial archaeology project.  ", "completion": "Code maintainability"}
{"prompt": "# How to measure code maintainability  Here are some simple ideas to get you started with measuring code maintainability:   * What percentage of your organization's codebase is searchable?  * What is the median lead time to make a change to part of the codebase to which I don't have write access?  * What percentage of our codebase is duplicate code? What percentage is unused?  * What percentage of applications aren't using the most recent stable version of all the libraries they consume?  * How many different versions of each library do we have in production? What's the median? What is a good goal? How many versions are more than 1 year old?  * How often do teams upgrade their libraries? How long does it take to do this? When considering what to measure, there are three use cases to focus on:   * Managing technical and design debt  * Change management (including emergency changes)  * Patching vulnerabilities. As codebases grow, technical debt is a major concern. It's important to be able to refactor and re-architect code as organizations and the products they and their customers rely on evolve. For large codebases, this can be complex and painful without significant tool support. It is also important to be able to identify code that is unused, duplicated, has poor test coverage, or contains vulnerabilities. The first step is to ensure that your tooling enables you to establish and track metrics that identify areas for improvement and make it straightforward to take action safely.  The second step is change management. When someone makes a change to part of the codebase, to what extent does your tooling help you detect the impact of that change? If another team is impacted, how fast can they take action to remedy the problem, particularly if the fix lies in a different area of the codebase? When an emergency change must be made, how long does it take to get the necessary code changes into the codebase, tested, and released?  Establish and track metrics so you can track how long changes take to propagate through your processes. Then identify bottlenecks and work to improve your processes, adding tool support where appropriate. Watch out for 'emergency' processes that bypass validations and approvals in order to gain speed — the goal should be to have your regular process be both reliable and fast enough to be effective in an emergency.  Patching vulnerabilities is a particularly important change management scenario. When a vulnerability is discovered in a library, how long does it take to discover and patch software that uses the vulnerable versions of the library? If you're not sure, this is worth testing on a regular basis. Given the enormous potential costs of handling breaches and data and code exfiltration, and the frequency of such attacks, it is typically worth devoting significant resources to making sure third-party software you rely on is up-to-date and can be easily upgraded in the event of vulnerabilities being discovered.  ", "completion": "Code maintainability"}
{"prompt": "# DevOps tech: Continuous delivery  Continuous delivery is the ability to release changes of all kinds on demand quickly, safely, and sustainably. Teams that practice continuous delivery well are able to release software and make changes to production in a low-risk way at any time—including during normal business hours—without impacting users.  You can apply the principles and practices of continuous delivery to any software context, including the following:   * Updating services in a complex distributed system.  * Upgrading mainframe software.  * Making infrastructure configuration changes.  * Making database schema changes.  * Updating firmware automatically.  * Releasing new versions of a mobile app. When your team practices continuous delivery, you can answer 'yes' to the following questions:   * Is our software in a deployable state throughout its lifecycle?  * Do we prioritize keeping the software deployable over working on new features?  * Is fast feedback on the quality and deployability of the system we are working on available to everyone on the team?  * When we get feedback that the system is not deployable (such as failing builds or tests), do we make fixing these issues our highest priority?  * Can we deploy our system to production, or to end users, at any time, on demand? Continuous delivery is commonly conflated with continuous deployment, but they are separate practices. Continuous deployment is when teams try to deploy every code change to production as soon as possible. Continuous deployment works well for web services, but can't be applied to software such as firmware or mobile apps. Continuous delivery is applied to all kinds of software including firmware and mainframe systems, and  in highly regulated environments. You can and should start with continuous delivery, even if you never intend to start using continuous deployment.  Continuous delivery and continuous deployment are mistakenly viewed as risky and not suited to regulated or safety critical domains. In fact, the goal of continuous delivery is to reduce software risk, and DORA research has shown consistently that high performers achieve higher levels of reliability and availability. The technical practices that drive continuous delivery—continuous testing, shifting left on security, and comprehensive testing and observability—are even more important in highly regulated and safety-critical domains. Continuous delivery has been successfully applied many times in highly regulated domains such as financial services and government.  Although continuous delivery is possible for all kinds of software, it is hard work. Nevertheless it provides significant benefits. DevOps Research and Assessment (DORA) research shows that doing well at continuous delivery provides the following benefits:   * Improves software delivery performance, measured in terms of the four key metrics, as well as higher levels of availability.  * Leads to higher levels of quality, measured by the percentage of time teams spend on rework or unplanned work (as shown in the 2016 State of DevOps Report  pp25-26, and the 2018 State of DevOps Report  pp27-29).  * Predicts lower levels of burnout (physical, mental, or emotional exhaustion caused by overwork or stress), higher levels of job satisfaction, and better organizational culture.  * Reduces deployment pain, a measure of the extent to which deployments are disruptive rather than easy and pain-free, as well as the fear and anxiety that engineers and technical staff feel when they push code into production.  * Impacts culture, leading to greater levels of psychological safety and a more mission-driven organizational culture. The following diagram shows how a set of technical practices impacts continuous delivery, which in turn drives the outcomes listed above:     Continuous delivery is only one aspect of driving the previously discussed outcomes, albeit a critical one. Other cultural and organizational capabilities discussed in the DORA research program  also help drive these outcomes. You can achieve continuous delivery by implementing the technical practices described in this document.  ", "completion": "Continuous delivery"}
{"prompt": "# Implementing continuous delivery  DORA research found that the following technical capabilities drive the ability to achieve continuous delivery. Transformational leadership within the organization also drives the implementation of many of these technical capabilities.  To help your team get higher throughput and lower risk releases, implement the following continuous delivery practices:   * Test automation: The use of comprehensive automated test suites primarily created and maintained by developers. Effective test suites are reliable—that is, tests find real failures and only pass releasable code.  * Deployment automation: The degree to which deployments are fully automated and do not require manual intervention.  * Trunk-based development: Characterized by fewer than three active branches in a code repository; branches and forks having very short lifetimes (e.g., less than a day) before being merged into mainline; and application teams rarely or never having code lock periods when no one can check in code or do pull requests due to merging conflicts, code freezes, or stabilization phases.  * Shift left on security: Integrating security into the design and testing phases of the software development process. This process includes conducting security reviews of applications, including the information security team in the design and demonstration process for applications, using pre-approved security libraries and packages, and testing security features as a part of the automated test suite.  * A loosely coupled architecture: Architecture that lets teams test and deploy their applications on demand, without requiring orchestration with other services. Having a loosely coupled architecture allows your teams to work independently without relying on other teams for support and services, which in turn enables them to work quickly and deliver value to the organization.  * Empowering teams to choose tools: Teams that can choose which tools to use do better at continuous delivery. No one knows better than practitioners what they need to be effective.  * Continuous integration (CI): A development practice where code is regularly checked in, and each check-in triggers a set of quick tests to discover regressions, which developers fix immediately. The CI process creates canonical builds and packages that are ultimately deployed and released.  * Continuous testing: Testing throughout the software delivery lifecycle rather than as a separate phase after dev complete. With continuous testing, developers and testers work side by side. High performers practice test-driven development, get feedback from tests in less than ten minutes, and continuously review and improve their test suites (for example, to better find defects and keep complexity under control).  * Version control: The use of a version control system, such as Git or Subversion, for all production artifacts, including application code, application configurations, system configurations, and scripts for automating build and configuration of environments.  * Test data management: Effective practices include having adequate data to run your test suite, the ability to acquire necessary data on demand, and the data not limiting the number of tests you can run. We caution that your teams should minimize, whenever possible, the amount of test data needed to run automated tests.  * Comprehensive monitoring and observability: Allows teams to understand the health of their systems. Effective solutions enable teams to monitor predefined metrics, including system state as experienced by users, as well as allowing engineers to interactively debug systems and explore properties and patterns as they emerge.  * Proactive notifications: Monitoring system health so that teams can preemptively detect and mitigate problems.  * Database change management: Database changes don't slow teams down if they follow a few key practices, including storing database changes as scripts in version control (and managing these changes the same way as production application changes), making database changes visible to everyone in the software delivery lifecycle (including engineers), and communicating with all parties when changes to the application require database changes.  * Code maintainability: Systems and tools that make it easy for developers to change code maintained by others, to find examples in the codebase, to reuse other people's code, and to add, upgrade, and migrate to new versions of dependencies without breaking their code. While continuous delivery is often combined with continuous integration and shortened to CI/CD, research shows that continuous integration is only one element of implementing continuous delivery. To achieve reliable, low-risk releases, you need close collaboration between everyone involved in the software delivery process, not just software developers, and your team needs to adopt new ways of working and learn new skills.  ", "completion": "Continuous delivery"}
{"prompt": "# Common pitfalls of implementing continuous delivery  Some organizations mistakenly believe that they can implement continuous delivery by doing their existing deployment process more often. However, implementing the technical capabilities that drive continuous delivery typically requires significant process and architectural changes. Increasing the frequency of deployments without improving processes and architecture is likely to lead to higher failure rates and burned out teams.  Many descriptions of continuous delivery focus on the tooling and patterns, such as the deployment pipeline that takes changes from version control to production. However, using modern tooling without implementing the necessary technical practices and process change described in this document won't produce the expected benefits.  The following diagram shows the J curve that DORA research has found to be typical of transformation programs. To emerge from the bottom of the J curve, your team needs to include process redesign and simplification, architectural improvement, and capability and skills development, along with automation and tooling.     In the diagram, the following stages are labeled:   * At the beginning of the curve, teams begin transformation and identify quick wins.  * In an initial improvement, automation helps low performers progress to medium performers.  * In a decrease to efficiency—the bottom of the J curve—automation increases test requirements, which are dealt with manually. A large amount of technical debt blocks progress.  * As teams begin to come out of the curve, technical debt and increased complexity cause additional manual controls and layers of process around changes, slowing work.  * At the top of the curve, relentless improvement work leads to excellence and high performance. High and elite performers leverage expertise and learn from their environments to see increases in productivity. A good way to mitigate the impact of the J-curve is to complete a value stream mapping (VSM) exercise to discover and anticipate the effect of bottlenecks. In a VSM exercise you look at a single change as it progresses from version control to production:   * Consider the various processes each change goes through, such as automated and manual testing, security review, change management, and release to production.  * Measure the total time for the change to go from version control to released. For each process, measure the following:  The actual elapsed time and the value-add time (the time during which work is actually being done) involved in executing the process end-to-end. The percentage of time that work is sent back because it was not done right the first time. This metric is known as percentage complete and accurate, or %C/A.   * The actual elapsed time and the value-add time (the time during which work is actually being done) involved in executing the process end-to-end.  * The percentage of time that work is sent back because it was not done right the first time. This metric is known as percentage complete and accurate, or %C/A. The purpose of the value stream mapping exercise is to help you find inefficiencies in your process. As a team, create a diagram of how you want the value stream to look in six months, and set aside team capacity to work on implementing this future state. Identify and remove obstacles that have a process with a long elapsed time compared to the value-add time or that have a poor %C/A.  It is usually necessary to tackle significant process and architecture redesign as part of implementing a deployment pipeline. Because the deployment pipeline goes from check-in to release, it connects multiple teams. It's essential to have representatives from all teams complete a value stream mapping exercise, and to agree on a common toolchain and processes (for example for deployment to testing and production environments) as part of your future state.  Implementing continuous delivery is a process of continuous, daily improvement work, guided by the outcomes that you want to achieve. Tools and patterns are valuable, but only in service to this essential improvement work.  ", "completion": "Continuous delivery"}
{"prompt": "# Measuring continuous delivery  Ultimately, the goal of continuous delivery is to ensure that releases are performed in a low-risk way during normal business hours. The goal should be that nobody has to work outside of regular business hours to perform deployments or releases, so this is something that is important to measure.  How well you are doing at continuous delivery is reflected in the outcomes that you achieve. You can take the DORA DevOps quick check  to see how you're doing with the key continuous delivery metrics:   * Short lead times for both regular and emergency changes, with a goal of using your regular process for emergency changes.  * Low change fail rates.  * Short time to restore service in the event of outages or service degradations.  * Release frequencies that ensure that high-priority features and bug fixes are released in a timely manner. As discussed at the start of this document, implementing continuous delivery should also lead to lower levels of rework, reduced deployment pain, and less time spent doing rework and unplanned work.  ", "completion": "Continuous delivery"}
{"prompt": "# DevOps measurement: Work in process limits  When faced with too much work and too few people to do it, rookie managers assign people to work on multiple tasks in the hope of increasing throughput. Unfortunately, the result is that tasks take longer to get done, and the team burns out in the process.  Instead, you should do the following:   * Prioritize work  * Limit how much people work on  * Focus on completing a small number of high-priority tasks The manufacturing sector has a long history of limiting the amount of work in process (WIP). Factories don't hold large amounts of inventory. Instead, when a customer orders a product, parts are made in-house, on-demand or are pulled from suppliers upstream as needed, and the company then assembles the product just in time. When you implement this process correctly, you end up with shorter lead times, higher quality, lower costs, and less waste.  ", "completion": "DevOps measurement: Work in process limits"}
{"prompt": "# How to implement work in process limits  Use a storyboard. In technology, our inventory is invisible. There's no shop floor with piles of work or assembly line where we can see the progression of work. A simple way to see inventory is to write all the work the team is doing on index cards and stick them on a board. In agile methods, this is called creating a storyboard.  The following sample storyboard spans multiple functions (analysis, development, testing, operations) and shows all work for a single product.    (Source: 'Kanban for Ops' board game, Dominica DeGrandis 2013')  A common practice with storyboards is to ink a dot onto a card for every day the card has been worked on. The team can easily see which work is blocked or taking longer than it should.  Specify limits. For each column on the board, specify the WIP limit, or how many cards can be in that column at one time. After the WIP limit is reached, no more cards can be added to the column, and the team must wait for a card to move to the next column before pulling the highest priority one from the previous column.  Only by imposing WIP limits and following this pull-based process do you actually create a Kanban board.  Determine WIP limits by team capacity. For example, if you have four pairs of developers, don't allow more than four cards in the 'in development' column.  Stick to the limits. WIP limits can result in teams sitting idle, waiting for other tasks to be completed. Don't increase WIP limits at this point. Instead, work to improve your processes to address the factors that are contributing to these delays. For example, if you're waiting for an environment to test your work, you might offer to help the team that prepares environments improve or streamline their process.  ", "completion": "DevOps measurement: Work in process limits"}
{"prompt": "# DevOps measurement: Monitoring systems to inform business decisions  Monitoring is the process of collecting, analyzing, and using information to track applications and infrastructure in order to guide business decisions. Monitoring is a key capability because it gives you insight into your systems and your work. Properly implemented, monitoring also gives you rapid feedback so that you can quickly find and fix problems early in the software development lifecycle.  Monitoring also helps you communicate information about your systems to people in other areas of the software development and delivery pipeline, and to other parts of the business. Knowledge acquired downstream in operations might get integrated into upstream teams, such as development and product management. For example, the knowledge gained from operating a highly scalable application that uses a NoSQL database as a data store can be valuable information for developers as they build a similar application.  This knowledge transfer allows teams to quickly identify learnings, whether they stem from a production issue, a deployment error, or your customer usage patterns. You can then share these learnings across your organization to help people and systems improve.  ", "completion": "DevOps measurement: Monitoring systems to inform business decisions"}
{"prompt": "# Common pitfalls with work in process limits  Organizations implementing WIP limits often encounter the following pitfalls:   * Not counting invisible work. It's important to visualize the whole value stream from idea to customer, not just the portion of the work that the team is responsible for. Without doing this, it's impossible to see the actual bottlenecks, and you'll end up addressing problems that aren't actually significant constraints to the flow of work. (This is also known as local optimums.)  * Setting WIP limits that are much too big. Make sure your WIP limits aren't too big. If your team is splitting their time between multiple tasks or projects, that's a good sign your WIP limits are too high.  * Relaxing WIP limits. Don't relax your WIP limits when people are idle. Instead, those people should be helping in other parts of the value stream, addressing the problems that are leading to constraints elsewhere.  * Quitting while you're ahead. If your WIP limits are easy to achieve, reduce them. The point of WIP limits is to expose problems in the system so they can be addressed. Another thing to look for is when there are too many columns on your visual display. Instead, look for ways to simplify the delivery process and reduce hand-offs. Process improvement work is key to increasing flow. DevOps Research & Assessment research shows that WIP limits help drive improvements in software delivery performance, particularly when they are combined with the use of visual displays  and feedback loops from monitoring.  ", "completion": "DevOps measurement: Work in process limits"}
{"prompt": "# Ways to improve work in process limits   * Make your work visible. As you do this, try to surface all of your work, making all of it visible, to several teams and stakeholders. (See visual displays  for details).  * Set WIP limits that match your team's capacity for work.  Account for activities like production support, meeting time and technical debt. Don't allow more WIP in any given part of the process than you have people to work on tasks. Don't require people to split their time between multiple tasks. When a particular piece of work is completed, move the card representing that work to the next column, and pull the highest priority piece of work waiting in the queue.   * Account for activities like production support, meeting time and technical debt.  * Don't allow more WIP in any given part of the process than you have people to work on tasks.  * Don't require people to split their time between multiple tasks.  * When a particular piece of work is completed, move the card representing that work to the next column, and pull the highest priority piece of work waiting in the queue.  * Set up a weekly meeting for stakeholders to prioritize all work in order. Let stakeholders know that if they don't attend, their work won't get done.  * Work to increase flow. Measure the lead time of work through the system. Record the date that work started on a card and the date work ended. From this information, you can create a running frequency histogram, which shows the number of days work takes to go through the system. This data will allow you to calculate the mean lead time, as well as variability, with the goal of having low variability: high variability means you are not scoping projects well or have significant constraints outside of your team. High variability also means your estimates and predictions about future work will not be as reliable.  * Improve work processes. Reduce hand-offs, simplify and automate tasks, and think about how to collaborate better to get work done. After you've removed some obstacles and things feel comfortable, reduce your WIP limits to reveal the next set of obstacles. The ideal is single-piece flow, which means that work flows from idea to customer with minimal wait time or rework. This ideal may not be achievable, but it acts as a 'true north' to guide the way in a process of continuous improvement. ", "completion": "DevOps measurement: Work in process limits"}
{"prompt": "# Ways to measure work in process limits  WIP limits are something you impose rather than measure, but it's important to keep finding ways to improve. During your regular retrospectives, ask the following questions:   * Do we know the mean lead time and variability for our entire value stream (from idea to customer)?  * Are we finding ways to increase flow and thus reduce lead time for work?  * Are our WIP limits surfacing obstacles that prevent us increasing flow?  * Are we doing things about those obstacles? ", "completion": "DevOps measurement: Work in process limits"}
{"prompt": "# DevOps measurement: Visual management capabilities  It's a common practice for teams that are adopting lean development practices  to display key information about their processes in team areas where everybody can see it. Visual management boards can create a shared understanding of where the team is in terms of its operational effectiveness. They can also help identify and remove obstacles in the path to higher performance.  ", "completion": "DevOps measurement: Visual management capabilities"}
{"prompt": "# How to implement visual management  There are many kinds of visual displays and dashboards that are common in the context of software delivery:   * Card walls, storyboards or Kanban boards, either physical or virtual, with index cards  that represent in-progress work items.  * Dashboards or other visual indicators, such as continuous integration systems with monitors or traffic lights to show whether the build is passing or failing. Effective visual displays are created, updated, and perhaps discarded by teams in response to issues that the team is currently interested in addressing.  * Burn-up or burn-down charts (for example, cumulative flow diagrams) showing the cumulative status of all work being done. These allow the team to project how long it will take to complete the current backlog.  * Deployment pipeline monitors showing what the latest deployable build is, and whether stages in the pipeline are failing, such as acceptance tests or performance tests.  * Monitors showing production telemetry, such as the number of requests being received, latency statistics, cumulative 404 and 500 errors, and which pages are most popular. When combined with the use of WIP limits  and using feedback from production to make business decisions, visual management displays can contribute to higher levels of delivery performance  (PDF).  ", "completion": "DevOps measurement: Visual management capabilities"}
{"prompt": "# Common pitfalls with visual management  The most important characteristics of visual management displays are that the team cares about and will act upon the information, and that the display is used during daily work to identify and remove obstacles to higher performance. Common pitfalls when implementing visual management include the following:   * Selecting metrics without the involvement of the team. Visual displays that show metrics that are highly relevant and useful to teams will be used more often. In addition, if teams can have input into the metrics that are displayed on their visual displays by participating in selecting their goals (for example, some teams use OKRs), they will be more motivated to drive progress toward those goals.  * Creating displays that are complex, hard to understand, or do not provide actionable information. It's easy to create displays using tools that allow high levels of modification or that are fun to play with. But changing layouts and color on a custom dashboard isn't helpful if the team is working with the wrong metrics or it takes the team several months to implement. Key metrics or rough graphs drawn on a whiteboard and updated daily can be just as effective to keep the team informed.  * Not evolving visual displays. Visual management tools should provide teams with information that addresses issues they are facing right now. It doesn't help one team to copy the displays of other teams unless the teams work in the same context, with the same challenges and obstacles. As a team's context evolves, visual displays should change as well. Also note that as teams address obstacles, their visual displays might change to discard old (previously relevant) metrics and highlight new areas of importance.  * Not addressing the underlying problem that the visual display is revealing. Teams sometimes make quick fixes in an effort to make the display 'green' again. Displays should be used to drive improvements (fix the problem), not become a goal in themselves (keep the board green). Focusing on managing the metric alone leads to unintended consequences and technical debt. If the display suggests a problem, teams should not just fix the immediate problem. They should also work to identify the underlying issue or constraint and resolve it, even if it's in another part of the organization. Any inefficiencies will keep showing up, and fixing them earlier will help all teams. ", "completion": "DevOps measurement: Visual management capabilities"}
{"prompt": "# Ways to improve visual management  The goal of visual management tools is to provide fast, easy-to-understand feedback so you can build quality into the product. This feedback helps the team identify defects in the product and understand whether some part of the system is not performing effectively, which helps them address the problem. In order to be effective, such systems must do the following:   * Reflect information that the team cares about and will act on. Having build monitors does no good if teams don't care whether the display shows an issue (for example, showing that the build status is red, meaning broken), and won't actually act on this information by swarming to fix the issue.  * Be easy to understand. It should be possible to tell at a glance from across the room whether something needs attention. If there is a problem, teams should know how to perform further diagnosis or fix the problem.  * Give the team information that is relevant to their work. While it's important to collect as much data as possible about the team's work, the display should present only data that is relevant to the team's goals. In the face of information overload, particularly information that cannot be acted upon, people ignore visual management displays; the displays just become noise. The additional data can be accessed and used by the team when they are swarming to fix the problem.  * Be updated as part of daily work. If the team lets the data go stale or become inaccurate, they will ignore the visual displays, and the displays will no longer be a useful beacon when important issues arise. If displays are currently displaying stale or inaccurate data, investigate the cause: is the data not related to the team's goals? What data would make the display an important and compelling information source for the team? Teams shouldn't get caught up in aspects of visual displays that aren't critical. For example, visual management displays don't need to be electronic. Physical card walls or kanban boards can be easier to manage and understand, particularly if the team is all in one location. These displays can also help develop valuable team rituals such as physically standing in front of the board to pick up work and move it around. A whiteboard with some key project information that is updated daily by the team is often preferable to an electronic system that's hard to understand, difficult to update, or doesn't have necessary information.  ", "completion": "DevOps measurement: Visual management capabilities"}
{"prompt": "# Ways to measure visual management  As with all improvement work, start with the measurable system-level goals that the team is working toward. Discover the existing state of the work system. Find a way to display the key information about the existing state, as well as the state you want. Make sure that this information is displayed only to the required precision.  Review the visual displays as part of regular retrospectives. Ask these questions:   * Are the displays giving you the information you need?  * Is the information up to date?  * Are people acting on this information?  * Is the information (and the actions people take in response to it) contributing to measurable improvement towards a goal that the team cares about?  * Does everybody know what the goals are?  * Can you look at your visual management displays and see the key process metrics you care about? If the answer to any of these questions is no, investigate further:   * Can you change the information or how it's displayed?  * Can you get rid of the display altogether?  * Can you create a new display? What would a prototype look like? What are the most important pieces of information to include, and how precise do they need to be to help you solve your problems and achieve your goals? ", "completion": "DevOps measurement: Visual management capabilities"}
{"prompt": "# DevOps measurement: Proactive failure notification  Proactive failure notification is the practice of generating notifications when monitored values approach known failure thresholds, and not waiting for the system to alert you it has already failed — or worse, to find out from customers that your application or service is down. Using this approach, you can identify and potentially resolve issues before they become serious or start to impact your users. The 2014 DevOps Research and Assessment (DORA)  (PDF) research showed that proactive monitoring is a significant predictor of software delivery performance. According to DORA research, teams that use proactive notification can diagnose and solve problems quickly. When failures are instead primarily reported by a source external to the operations team, such as by the network operations center (NOC) — or worse, by customers — rather than internal monitoring, performance suffers.  ", "completion": "DevOps measurement: Proactive failure notification"}
{"prompt": "# How to implement proactive failure notification  Use alerting rules. You should generate failure notifications using specific alerting rules. Alerting rules define the conditions under which an alert is generated and the notification channel for that alert. Read more about  generating alerts in Monitoring and observability.  Use thresholds. Alerting rules should use thresholds for the metrics you monitor that indicate real trouble. Monitoring thresholds trigger alerting rules, which generate notifications when metric levels cross threshold values.  Choose thresholds carefully. Choose thresholds to only generate alerts when the threshold actually predicts an issue. That is, don't arbitrarily select a value. Generally, you should identify which value levels begin to cause user-facing impact, and then trigger an alert notification at some percentage before that value is crossed.  For example, you might choose to trigger an alert notification when average response time for pages is within 20% of a threshold at which you know users start becoming frustrated and calling support.  Hold incident post-mortems. When you hold post-mortems following incidents, determine which indicators could have predicted the incident and monitor them in the future.  Plan a notification strategy. If a notification requires no action or the same action every time, you should automate the response. You should also consider the volume of notifications for events. A deluge of notifications during an event might be distracting rather than useful. When people are exposed to a large number of alarms, they can become desensitized to them (a problem known as 'alert fatigue') leading to longer response times or missed alarms. Regularly review notifications and delete those that cannot be acted upon.  ", "completion": "DevOps measurement: Proactive failure notification"}
{"prompt": "# Ways to improve failure notification  Configure alerts to notify your key teams when something goes wrong in your systems early, long before it moves to the Network Operations Center (NOC) or to a customer. Tactics include:   * Configuring alerts in logging and monitoring systems to appropriate levels.  * Configuring alerts to make sure they notify people and teams who can fix the problem.  * Proactively monitoring system health based on threshold warnings before system failures happen.  * Proactively monitoring system health based on rate of change warnings.  * Ensuring that only relevant alerts are occurring, and that the team isn't receiving too many alerts. Take a hard look at which alerts are irrelevant. Disable irrelevant alerts and turn relevant monitoring alerts back on. Disabling all alerts is bad practice. ", "completion": "DevOps measurement: Proactive failure notification"}
{"prompt": "# Ways to measure failure notifications  Instrumenting proactive monitoring is straightforward. The components to capture are:   * The extent to which failure alerts from logging and monitoring systems are captured and used.  * The extent to which system health is proactively monitored using threshold warnings.  * The extent to which system health is proactively monitored using rate of change warnings. To make sure you are capturing different aspects of your system, you should monitor metrics in at least two different ways. For example, you might set a metric threshold that triggers alerts if a metric rises or falls below a value over a given time window, and a rate of change, which triggers alerts when a metric value change rate is higher or lower than expected.  ", "completion": "DevOps measurement: Proactive failure notification"}
{"prompt": "# How to implement monitoring  The following elements are key to effective monitoring:   * Collecting data from key areas throughout the value chain, including application performance and infrastructure.  * Using the collected data to make business decisions. To collect data more effectively, you should implement monitoring solutions, either as homegrown services or managed services, that give visibility into development work, testing, QA, and IT operations. Make sure that you choose metrics that are appropriate for function and for your business.  When you transform and visualize the collected data, you make it accessible to different audiences and help them make decisions. For example, you might want to share operations data upstream.You can also integrate this data as appropriate into reports and briefings, and use it in meetings to make informed business decisions. In this case, appropriate means relevant, timely, accurate, and easy to understand.  In these meetings, be sure to also provide context, to help those who might not be familiar with the data understand how it pertains to the discussion and how it can inform the decisions to be made. For example, you might want to know how to answer the following questions:   * Are these values relatively high or low?  * Are they expected?  * Do you anticipate changes?  * How is this data different from historical reports?  * Has your technology or infrastructure impacted the numbers in interesting or non-obvious ways? ", "completion": "DevOps measurement: Monitoring systems to inform business decisions"}
{"prompt": "# Common pitfalls in monitoring  The following pitfalls are common when monitoring systems:   * Monitoring reactively. For example, only getting alerted when the system goes down, but not using monitoring data to help alert when the system approaches critical thresholds. Monitoring reactively. For example, only getting alerted when the system goes down, but not using monitoring data to help alert when the system approaches critical thresholds.   * Monitoring too small a scope. For example, monitoring one or two areas rather than the full software development and delivery pipeline. This pitfall highlights metrics, focusing only on the areas that are measured, which might not be the optimal areas to monitor. Monitoring too small a scope. For example, monitoring one or two areas rather than the full software development and delivery pipeline. This pitfall highlights metrics, focusing only on the areas that are measured, which might not be the optimal areas to monitor.   * Focusing on local optimizations. For example, focusing on reducing the response time for one service's storage needs without evaluating whether the broader infrastructure could also benefit from the same improvement. Focusing on local optimizations. For example, focusing on reducing the response time for one service's storage needs without evaluating whether the broader infrastructure could also benefit from the same improvement.   * Monitoring everything. By collecting data and reporting on everything your system, you run the risk of over-alerting or drowning in data. Taking a thoughtful approach to monitoring can help draw attention to key areas. Monitoring everything. By collecting data and reporting on everything your system, you run the risk of over-alerting or drowning in data. Taking a thoughtful approach to monitoring can help draw attention to key areas.  ", "completion": "DevOps measurement: Monitoring systems to inform business decisions"}
{"prompt": "# Ways to improve monitoring  To improve your monitoring effectiveness, we recommend that you focus your efforts on two main areas:   * Collecting data from key areas throughout the value chain. By analyzing the data that you collect and doing a gap analysis, you can help ensure that you collect the right data for your organization. Collecting data from key areas throughout the value chain.  By analyzing the data that you collect and doing a gap analysis, you can help ensure that you collect the right data for your organization.   * Using the collected data to make business decisions. The data that you collect should drive value across the organization, and the metrics that you select must be meaningful to your organization. Meaningful data can be used by many teams, from DevOps to Finance. It's also important to find the right medium to display the monitoring information. Different uses for the information demand different presentation choices. Real-time dashboards might be most useful to the DevOps team, while regularly generated business reports might be useful for metrics measured over a longer period. The most important thing is to ensure the data is available, shared, and used to guide decisions. If the best you can do to kick things off is a shared spreadsheet, use that. Then graduate to fancy dashboards later. Don't let perfect be the enemy of good enough. Using the collected data to make business decisions.  The data that you collect should drive value across the organization, and the metrics that you select must be meaningful to your organization. Meaningful data can be used by many teams, from DevOps to Finance.  It's also important to find the right medium to display the monitoring information. Different uses for the information demand different presentation choices. Real-time dashboards might be most useful to the DevOps team, while regularly generated business reports might be useful for metrics measured over a longer period.  The most important thing is to ensure the data is available, shared, and used to guide decisions. If the best you can do to kick things off is a shared spreadsheet, use that. Then graduate to fancy dashboards later. Don't let perfect be the enemy of good enough.  ", "completion": "DevOps measurement: Monitoring systems to inform business decisions"}
{"prompt": "# Ways to measure monitoring  Effective monitoring helps drive performance improvements in software development and delivery. However, measuring the effectiveness of monitoring can be difficult to instrument in systems. Although you might be able to automatically measure how much data is being collected from your systems and the types of that data, it's more difficult to know if or where that data is being used.  To help you gauge the effectiveness of monitoring in your organization, consider the extent to which people agree or disagree with the following statements:   * Data from application performance monitoring tools is used to make business decisions.  * Data from infrastructure monitoring tools is used to make business decisions. ", "completion": "DevOps measurement: Monitoring systems to inform business decisions"}
{"prompt": "# DevOps measurement: Monitoring and observability  Good monitoring is a staple of high-performing teams. DevOps Research and Assessment (DORA) research  shows that a comprehensive monitoring and observability solution, along with a number of other technical practices, positively contributes to continuous delivery.   DORA's research defined these terms as follows:  Monitoring is tooling or a technical solution that allows teams to watch and understand the state of their systems. Monitoring is based on gathering predefined sets of metrics or logs.  Observability is tooling or a technical solution that allows teams to actively debug their system. Observability is based on exploring properties and patterns not defined in advance.  To do a good job with monitoring and observability, your teams should have the following:   * Reporting on the overall health of systems (Are my systems functioning? Do my systems have sufficient resources available?).  * Reporting on system state as experienced by customers (Do my customers know if my system is down and have a bad experience?).  * Monitoring for key business and systems metrics.  * Tooling to help you understand and debug your systems in production.  * Tooling to find information about things you did not previously know (that is, you can identify unknown unknowns).  * Access to tools and data that help trace, understand, and diagnose infrastructure problems in your production environment, including interactions between services. ", "completion": "DevOps measurement: Monitoring and observability"}
{"prompt": "# How to implement monitoring and observability  Monitoring and observability solutions are designed to do the following:   * Provide leading indicators of an outage or service degradation.  * Detect outages, service degradations, bugs, and unauthorized activity.  * Help debug outages, service degradations, bugs, and unauthorized activity.  * Identify long-term trends for capacity planning and business purposes.  * Expose unexpected side effects of changes or added functionality. As with all DevOps capabilities, installing a tool is not enough to achieve the objectives, but tools can help or hinder the effort.  Monitoring systems should not be confined to a single individual or team within an organization. Empowering all developers to be proficient with monitoring helps develop a culture of data-driven decision making and improves overall system debuggability, reducing outages.  There are a few keys to effective implementation of monitoring and observability. First, your monitoring should tell you what is broken and help you understand why, before too much damage is done. The key metric in the event of an outage or service degradation is time-to-restore (TTR). A key contributor to TTR is the ability to rapidly understand what broke and the quickest path to restoring service (which may not involve immediately remediating the underlying problems).  There are two high-level ways of looking at a system: blackbox monitoring, where the system's internal state and mechanisms are not made known, and whitebox monitoring, where they are.  For more information, see 'Monitoring Distributed Systems' in the Site Reliability Engineering  book.  In a blackbox (or synthetic) monitoring system, input is sent to the system under examination in the same way a customer might. This might take the form of HTTP calls to a public API, or RPC calls to an exposed endpoint, or it might be calling for an entire web page to be rendered as a part of the monitoring process.  Blackbox monitoring is a sampling-based method. The same system that is responsible for user requests is monitored by the blackbox system. A blackbox system can also provide coverage of the target system's surface area.  This could mean probing each external API method. You might also consider a representative mixture of requests to better mimic actual customer behavior. For example, you might perform 100 reads and only 1 write of a given API.  You can govern this process with a scheduling system, to ensure that these inputs are made at a sufficient rate in order to gain confidence in their sampling. Your system should also contain a validation engine, which can be as simple as checking response codes, or matching output with regular expressions, up to rendering a dynamic site in a headless browser and traversing its DOM tree, looking for specific elements. After a decision is made (pass, fail) on a given probe, you must store the result and metadata for reporting and alerting purposes.  Examining a snapshot of a failure and its context can be invaluable for diagnosing an issue.  Monitoring and observability rely on signals sent from the workload under scrutiny into the monitoring system. This can generally take the form of the three most common components: metrics, logs, and traces. Some monitoring systems also track and report events, which can represent user interactions with an entire system, or state changes within the system itself.  Metrics are simply measurements taken inside a system, representing the state of that system in a measurable way. These are almost always numeric and tend to take the form of counters, distributions, and gauges. There are some cases where string metrics make sense, but generally numeric metrics are used due to the need to perform mathematical calculations on them to form statistics and draw visualizations.  Logs can be thought of as append-only files that represent the state of a single thread of work at a single point in time. These logs can be a single string like 'User pushed button X' or a structured log entry which includes metadata such as the time the event happened, what server was processing it, and other environmental elements.  Sometimes a system which cannot write structured logs will produce a semi-structured string like [timestamp] [server] message [code] which can be parsed after the fact, as needed. Log entries tend to be written using a client library like log4j, structlog, bunyan, log4net, or Nlog. Log processing can be a very reliable method of producing statistics that can be considered trustworthy, as they can be reprocessed based on immutable stored logs, even if the log processing system itself is buggy. Additionally, logs can be processed in real time to produce log-based metrics.  Traces are composed of spans, which are used to follow an event or user action through a distributed system. A span can show the path of a request through one server, while another span might run in parallel, both having the same parent span. These together form a trace, which is often visualized in a waterfall graph similar to those used in profiling tools. This lets developers understand time taken in a system, across many servers, queues, and network hops. A common framework for this is OpenTelemetry, which was formed from both OpenCensus  and OpenTracing.  Metrics, logs, and traces can be reported to the monitoring system by the server under measurement, or by an adjacent agent that can witness or infer things about the system.  To make use of a monitoring system, your system must be instrumented. That is, code must be added to a system in order to expose its inner state. For example, if a simple program contains a pool of connections to another service, you might want to keep track of the size of that pool and the number of unused connections at any given time. In order to do so, a developer must write some code in the connection pool logic to keep track of when connections are formed or destroyed, when they are handed out, and when they are returned. This might take the form of log entries or events for each of these, or you might increment and decrement a gauge for the size of the queue, or you might increment a counter each time a connection is created, or each time a pool is expanded.  Metrics can be collected from the application, as well as from its underlying systems, such as the JVM, guest OS, hypervisor, node OS, and the hardware itself. Note that as you go further down in a stack, you might start conflating metrics that are shared across workloads. For example, if a single machine serves several applications, watching the disk usage might not correspond directly to the system under observation. Correlating issues across applications on a shared system, however, can help you pin down a contributing factor (such as a slow disk). Drilling down from a single application to its underlying system metrics, then pulling up to show all similarly affected applications can be very powerful.  Measuring a distributed system means having observability in many places and being able to view them all together. This might mean both a frontend and its database, or it might mean a mobile application running on a customer's device, a cloud load balancer, and a set of microservices. Being able to connect data from all of these sources in one place is a fundamental requirement in modern observability tools.  After you collect data from various sources for your system, you generate statistics and aggregate data across various realms. This might be cohorts of users, regions of your compute footprint, or geographic locations of your customers. Being able to develop these statistics on-the-fly based on raw events is very advantageous but can be costly both in terms of storage as well as real-time compute capacity required.  When you choose your tooling and plan your instrumentation, you must consider cardinality and dimensionality. These two aspects of metric collection can greatly affect your ability to scale your ability to observe a system.  Cardinality is the measure of distinct values in a system. For example, a field like cpu-utilization tends to need a range between 0 and 100. However, if you keep track of a user's unique identifier, they're all distinct, thus if you have 1M users, you have a cardinality of 1M. This makes a huge difference.  Dimensionality is the ability to record more than just a single value along with a timestamp, as you might have in a simple timeseries database that backs a monitoring system. Simply recording the value of a counter, for example requests-sent, might initially record just the value of the number of requests sent up to this point in time like {time=x, value=y}. However, as with structured logs, you might want to also record some environmental data, resulting in something like: {time=x, value=y, server=foo, cluster=123, environment=prod, service=bar}. Combining high cardinality and high dimensionality can result in dramatically increased compute and storage requirements, to the point where monitoring might not work as expected!  This needs to be understood by developers who write dynamically generated data and metadata into monitoring systems.  Part of operating a system is learning from outages and mistakes. The process of writing retrospectives or postmortems with corrective actions is well documented. One outcome of this process is the development of improved monitoring. It is critical to a fast-moving organization to allow their monitoring systems to be updated quickly and efficiently by anyone within the organization. Monitoring configuration is critical, so changes should be tracked by means of review and approval, just as with code development and delivery. Keeping your monitoring configuration in a version control system  is a good first step in allowing broad access to the system, while maintaining control on this critical part of your system. Developing automation around deploying monitoring configuration through an automation pipeline can also improve your ability to ensure these configurations are valid and applied consistently. After you treat your monitoring configuration as code, these improvements can all be accomplished by means of a deployment automation  process, ideally the same system used by the rest of your team.  ", "completion": "DevOps measurement: Monitoring and observability"}
{"prompt": "# Common pitfalls of implementing monitoring and observability  When developing a system for monitoring and observability for your organization, you should be aware that there generally isn't a simple plug-and-play solution. Any decent monitoring system will require a deep understanding of each component that you want to measure, as well as direct manipulation of the code to instrument those systems. Avoid having a single monitoring person or dedicated team who is solely responsible for the system. This will not only help you prevent a single point of failure, but also increase your ability to understand and improve your system as an entire organization. Monitoring and observability needs to be built into the baseline knowledge of all your developers. A common pitfall here is for the operation team, NOC, or other similar team to be the only ones allowed to make changes to a monitoring system. This should be avoided and replaced with a system that follows CD patterns.  A common anti-pattern in writing alerts in monitoring systems is to attempt to enumerate all possible error conditions and write an alert for each of them. We call this cause-based alerting, and you should avoid it as much as possible. Instead, you should focus on symptom-based alerting, which only alerts you when a user-facing symptom is visible or is predicted to arise soon. You should still be able to observe non-user-facing systems, but they shouldn't be directly alerting on-call engineers if there are no user-facing symptoms. Note that the term user-facing can also include users internal to your organization.  When generating alerts, you should consider how they are delivered. Your alerts should have multiple pathways to oncall engineers, including but not limited to: SMS delivery, dedicated mobile apps, automated phone calls, or email. A common pitfall is to email alerts to an entire team via an email distribution list. This can quickly result in ignored alerts due to diffusion of responsibility. Another common failure is simply a poor signal-to-noise ratio. If too many alerts are not actionable, or result in no improvement, the team will easily miss those alerts that are meaningful and possibly very important, a problem known as alarm fatigue. Any method to silence or suppress some set of alerts should be tracked very carefully to ensure it is not too broad or applied too often.  When building monitoring dashboards to visualize metrics, a common mistake is to spend a long time curating the 'Perfect Dashboard'. This is similar to the cause-based alerting mistake above. In a high-functioning team, the system under observation changes so fast that any time spent curating a dashboard will be out of date before it is done. Instead, focusing on the ability for team members to quickly create a dashboard or other set of visualizations that fits their needs is important.  Failing to separate out product or executive-facing metrics such as user acquisition rate and revenue tracking away from operational or service health dashboards is also a common problem, as they are both very important, but distinct. Keeping these separate is highly recommended.  ", "completion": "DevOps measurement: Monitoring and observability"}
{"prompt": "# How to measure monitoring and observability  When implementing a monitoring and observability system in your organization, you can track some internal metrics to see how well you're doing.  Here are some that you might wish to track with a monthly survey, or possibly by automatically analyzing your postmortems or alerting logs.   * Changes made to monitoring configuration. How many pull requests or changes per week are made to the repository containing the monitoring configuration? How often are these changes pushed to the monitoring system? (Daily? Batched? Immediately on PR?)  * 'Out of hours' alerts. What percentage of alerts are handled at night? While some global businesses have a follow-the-sun support model which makes this a non-issue, it can be an indication that not enough attention has been paid to leading indicators of failures. Regular night-time alerts can lead to alert fatigue and burned-out teams.  * Team alerting balance. If you have teams in different locations responsible for a service, are alerts fairly distributed and addressed by all teams?  If not, why?  * False positives. How many alerts resulted in no action, or were marked as 'Working as Intended'? Alerts which aren't actionable and which haven't helped you predict failures should be deleted.  * False negatives. How many system failures happened with no alerting, or alerting later than expected? How often do your postmortems include adding new (symptom-based) alerts?  * Alert creation. How many alerts are created per week (total, or grouped by severity, team, etc.)?  * Alert acknowledgement. What percentage of alerts are acknowledged within the agreed deadline (such as 5 minutes, 30 minutes)? Sometimes this is coupled with or can be tracked by a metric like alert fall-through when a secondary oncall person is notified for an alert.  * Alert silencing and silence duration. How many alerts are in a silenced or suppressed state per week?  How many are added to this pool, how many removed?  If your alert silencing has an expiration system, how many silences are extended to last longer than initially expected?  What is the mean and maximum silence period? (A fun one is 'how many silences are effectively 'infinite''?)  * Unactionable alerts. What percentage of alerts were considered 'unactionable'? That is, the engineer alerted was not able to immediately take some action, either out of an inability to understand the alert implication, or due to a known issue. Unactionable alerts are a well known source of toil.  * Usability: alerts, runbooks, dashboards. How many graphs are on your dashboards? How many lines per graph? Can teams understand the graphs? Is there explanatory text to help out new engineers? Do people have to scroll and browse a lot to find the information they need? Can engineers navigate from alert to playbook to dashboards effectively? Are the alerts named in such a way to point engineers in the right direction? These might be measured by surveys of the team, over time.  * MTTD, MTTR, impact. The bottom line is time to detect, time to resolve, and impact. Consider measuring the 'area under the curve' of the time that the outage was affecting customers times the number of customers affected. This can be estimated or done more precisely with tooling. By tracking some or all of these metrics, you'll start to gain a better understanding of how well your monitoring and observability systems are working for your organization. Breaking these measurements down by product, by operational team, or other methods will give you insight not only into the health of your products but also your processes and your people.  ", "completion": "DevOps measurement: Monitoring and observability"}
{"prompt": "# DevOps process: Working in small batches  Working in small batches is an essential principle in any discipline where feedback loops are important, or you want to learn quickly from your decisions. Working in small batches allows you to rapidly test hypotheses about whether a particular improvement is likely to have the effect you want, and if not, lets you course correct or revisit assumptions. Although this article applies to any type of change that includes organizational transformation and process improvement, it focuses primarily on software delivery.  Working in small batches is part of lean product management. Together with capabilities like visibility of work in the value stream, team experimentation, and visibility into customer feedback, working in small batches predicts software delivery performance and organizational performance.  One reason work is done in large batches is because of the large fixed cost of handing off changes. In traditional phased approaches to software development, handoffs from development to test or from test to IT operations consist of whole releases: months worth of work by teams consisting of tens or hundreds of people. With this traditional approach, collecting feedback on a change can take weeks or months.  In contrast, DevOps practices, which utilize cross-functional teams and lightweight approaches, allow for software to progress from development through test and operations into production in a matter of minutes. However, this rapid progression requires working with code in small batches.  Working in small batches has many benefits:   * It reduces the time it takes to get feedback on changes, making it easier to triage and remediate problems.  * It increases efficiency and motivation.  * It prevents your organization from succumbing to the sunk-cost fallacy. You can apply the small batches approach at the feature and the product level. As an illustration, a minimum viable product, or MVP, is a prototype of a product with just enough features to enable validated learning about the product and its business model.  Continuous delivery builds upon working in small batches and tries to get every change in version control as early as possible. A goal of continuous delivery is to change the economics of the software delivery process, making it viable to work in small batches. This approach provides fast, comprehensive feedback to teams so that they can improve their work.  ", "completion": "DevOps process: Working in small batches"}
{"prompt": "# How to work in small batches  When you plan new features, try to break them down into work units that can be completed independently and in short timeframes. We recommend that each feature or batch of work follow the agile concept of the INVEST principle:   * Independent. Make batches of work as independent as possible from other batches, so that teams can work on them in any order, and deploy and validate them independent of other batches of work.  * Negotiable. Each batch of work is iterable and can be renegotiated as feedback is received.  * Valuable. Discrete batches of work are usable and provide value to the stakeholders.  * Estimable. Enough information exists about the batches of work that you can easily estimate the scope.  * Small. During a sprint, you should be able to complete batches of work in small increments of time, meaning hours to a couple days.  * Testable. Each batch of work can be tested, monitored, and verified as working in the way users expect. When features are of an appropriate size, you can split the development of the feature into even smaller batches. This process can be difficult and requires experience to develop. Ideally, your developers should be checking multiple small releasable changes into trunk at least once per day.  The key is to start development at the service or API layer, not at the UI layer. In this way, you can make additions to the API that won't initially be available to users of the app, and check those changes into trunk. You can launch these changes to production without making them visible to users. This approach, called dark launching, allows developers to check in code for small batches that have been completed, but for features that are not yet fully complete. You can then run automated tests  against these changes to prove that they behave in the expected way. This way, teams are still working quickly and developing off of trunk  and not long-lived feature branches.  You can also dark launch changes by using a feature toggle, which is a conditional statement based on configuration settings. For example, you can make UI elements visible or invisible, or you can enable or disable service logic. Feature-toggle configuration might be read either at deploy time or runtime. You can use these configuration settings to switch the behavior of new code further down the stack. You can also use similar technique known as branch by abstraction  to make larger-scale changes to the system while continuing to develop and release off-trunk without the use of long-lived feature branches.  In this approach, batches of work aren't complete until they're deployed to production and the feedback process has begun to validate the changes. Feedback comes from many sources and in many forms, including users, system monitoring, quality assurance, and automated tests. Your goal is to optimize for speed so that you reduce the cycle time to get changes into the hands of users. This way, you can validate your hypothesis as quickly as possible.  ", "completion": "DevOps process: Working in small batches"}
{"prompt": "# Common pitfalls with working in small batches  When you break down work into small batches, you encounter two pitfalls:   * Not breaking up work into small enough pieces. Your first task is to break down the work in a meaningful way. We recommend that you commit code independent of the status of the feature and that individual features take no more than a few days to develop. Any batch of code that takes longer than a week to complete and check is too big. Throughout the development process, it's essential that you analyze how to break down ideas into increments that you can develop iteratively. Not breaking up work into small enough pieces. Your first task is to break down the work in a meaningful way. We recommend that you commit code independent of the status of the feature and that individual features take no more than a few days to develop. Any batch of code that takes longer than a week to complete and check is too big. Throughout the development process, it's essential that you analyze how to break down ideas into increments that you can develop iteratively.   * Working in small batches but then regrouping the batches before sending them downstream for testing or release. Regrouping work in this way delays the feedback on whether the changes have defects, and whether your users and your organization agree the changes were the right thing to build in the first place. Working in small batches but then regrouping the batches before sending them downstream for testing or release. Regrouping work in this way delays the feedback on whether the changes have defects, and whether your users and your organization agree the changes were the right thing to build in the first place.  ", "completion": "DevOps process: Working in small batches"}
{"prompt": "# Ways to reduce the size of work batches  When you slice work into small batches that can be completed in hours, you can typically test and deploy those batches to production in less than an hour  (PDF). The key is to decompose the work into small features that allow for rapid development, rather than developing complex features on branches and releasing them infrequently.  To improve small batch development, check your environment and confirm that the following conditions are true:   * Work is decomposed in a way that enables teams to make more frequent production releases.  * Developers are experienced in breaking down work into small changes that can be completed in the space of hours, not days. To become an expert in small batch development, strive to meet each of these conditions in all of your development teams. This practice is a necessary condition for both continuous integration  and trunk-based development.  ", "completion": "DevOps process: Working in small batches"}
{"prompt": "# Ways to measure the size of work batches  When you understand continuous integration  and monitoring, you can outline possible ways to measure small batch development in your systems and development environment.   * Application features are decomposed in a way that supports frequent releases. How often are releases possible? How does this release cadence differ across teams? Are delays in production related to features that are larger?  * Application features are sliced in a way that lets developers complete the work in one week or less. What proportion of features can you complete in one week or less? What features can't you complete in one week or less? Can you commit and release changes before the feature is complete?  * MVPs are defined and set as goals for teams. Is the work decomposed into features that allow for MVPs and rapid development, rather than complex and lengthy processes? Your measurements depend on the following:   * Knowing your organization's processes.  * Setting goals for reducing waste.  * Looking for ways to reduce complexity in the development process. ", "completion": "DevOps process: Working in small batches"}
{"prompt": "# Common pitfalls with work visibility  Some common obstacles to implementing work visibility include the following:   * Overestimating the state of organizational knowledge. In any organization, nobody has a good view into the whole value stream. When a company puts together a value stream mapping exercise, it's important to gather people from across the value stream to perform the exercise. There is often surprise as people find out what actually goes on in other parts of the organization. Overestimating the state of organizational knowledge. In any organization, nobody has a good view into the whole value stream. When a company puts together a value stream mapping exercise, it's important to gather people from across the value stream to perform the exercise. There is often surprise as people find out what actually goes on in other parts of the organization.   * Failing to map the entire value stream. It's important to map the complete value stream from idea (whether that's a line of business, the product marketing department, or internal customers) through to IT operations and the people who support the product or service being mapped. The increased visibility and alignment that results, along with the shared understanding, are extremely valuable. Failure to map the entire value stream can lead to local optimizations and missed opportunities to improve processes in key areas that have an impact on the entire organization. Failing to map the entire value stream. It's important to map the complete value stream from idea (whether that's a line of business, the product marketing department, or internal customers) through to IT operations and the people who support the product or service being mapped. The increased visibility and alignment that results, along with the shared understanding, are extremely valuable. Failure to map the entire value stream can lead to local optimizations and missed opportunities to improve processes in key areas that have an impact on the entire organization.   * Focusing on the wrong areas for improvement. Improving efficiency in areas that aren't bottlenecks won't have much impact on overall lead times, and can make things worse. Companies should of course improve in all areas, but there's no point in investing substantial effort in an effort that won't have organization-level outcomes. Focusing on the wrong areas for improvement. Improving efficiency in areas that aren't bottlenecks won't have much impact on overall lead times, and can make things worse. Companies should of course improve in all areas, but there's no point in investing substantial effort in an effort that won't have organization-level outcomes.   * Not granting authority to make changes. People involved in the effort must have the authority to make changes to achieve the future state. If these people must try to persuade others in the organization, the exercise is unlikely to succeed. Not granting authority to make changes. People involved in the effort must have the authority to make changes to achieve the future state. If these people must try to persuade others in the organization, the exercise is unlikely to succeed.  In addition, make sure you look at common obstacles in the articles on WIP limits  and visual management. Many of them apply in this context too.  ", "completion": "DevOps process: Visibility of work in the value stream"}
{"prompt": "# Ways to improve work visibility   * Provide tools for visualizing and recording workflow. Start with making sure the team has visual management  displays that show their work and its flow through the part of the value stream that is closest to them, including both the upstream and downstream parts of the process. Record how long it takes work to get through the process, and how often rework must be performed because the team didn't get it right the first time. This will uncover your early and best opportunities for improvement at the team level. Provide tools for visualizing and recording workflow. Start with making sure the team has visual management  displays that show their work and its flow through the part of the value stream that is closest to them, including both the upstream and downstream parts of the process. Record how long it takes work to get through the process, and how often rework must be performed because the team didn't get it right the first time. This will uncover your early and best opportunities for improvement at the team level.   * Create a value stream map. Work with other teams to perform a value-stream mapping exercise to discover how work flows from idea to customer outcome, and report the VSM metrics (lead time, process time, %C/A) for each process block. Have the team prepare a future-state value stream map and work to implement it. Create a value stream map. Work with other teams to perform a value-stream mapping exercise to discover how work flows from idea to customer outcome, and report the VSM metrics (lead time, process time, %C/A) for each process block. Have the team prepare a future-state value stream map and work to implement it.   * Share artifacts. Make sure the artifacts from these exercises are available to everyone in the organization, and that they are updated at least annually. Share artifacts. Make sure the artifacts from these exercises are available to everyone in the organization, and that they are updated at least annually.  ", "completion": "DevOps process: Visibility of work in the value stream"}
{"prompt": "# Ways to measure work visibility  To determine the effectiveness of the team's visibility to the work in the value stream, ask these questions:   * Is there a current or recent value stream map available to anyone in the organization?  * Does everybody in the organization have access to a visual display that shows what they're working on and the status of their work?  * Are statistics on metrics such as lead time and %C/A available to the team? ", "completion": "DevOps process: Visibility of work in the value stream"}
{"prompt": "# DevOps process: Customer feedback  In software projects, developers often work on products for months or years, sometimes for multiple releases without validating whether the features they're building are actually helping users solve their problems, or whether the features are being used at all.  Customer feedback is part of a wider group of capabilities, including visibility of work in the value stream, working in small batches, and team experimentation, that together represent a lean approach to product management.  When these capabilities are applied together, they help predict the following:   * Software delivery performance, which is measured in terms of delivery speed, stability, and availability.  * Organizational performance, which is measured in terms of profitability, market share, and productivity. DevOps Research and Assessment (DORA) research shows  (PDF) that teams achieve higher performance when they work in organizations that utilize those capabilities and also do the following:   * Collect customer satisfaction metrics regularly.  * Seek out and attend to customer feedback on product and feature quality.  * Use this feedback to help design products and features. ", "completion": "DevOps process: Customer feedback"}
{"prompt": "# How to implement customer feedback  When you are developing a product, it's important to establish key metrics to gauge its success. These metrics help you understand whether you're solving a real problem, whether your solution is being adopted sufficiently quickly, and whether users continue to use it and recommend it to others. These metrics must be explicitly derived from how customers interact with the product. Using these metrics requires you to carefully gather and analyze customer feedback.  One set of metrics that's popular in consumer-facing SaaS products is AARRR: acquisition, activation, retention, referral, and revenue. (The order is sometimes different depending on who is presenting them.) These are sometimes known as pirate metrics because the acronym spells out a word that's stereotypically associated with how pirates talk.  The idea is to look at five key metrics and iterate your customer experience to improve on them:   * Acquisition: The percentage of users that come to your site who create an account.  * Activation: The percentage of acquired users that activate their account and use the service.  * Retention: The percentage of activated users that return to the service.  * Referral: The percentage of retained users who refer other users to the service.  * Revenue: The percentage of referring users who actually pay money for the service. If these metrics aren't suitable for your business or product, it's essential to discover some that are suitable, monitor those metrics regularly, and use them as a key driver of your product strategy.  The approach described in this article is not just for external-facing products. It applies equally to internal products and services that are built for other users in your organization. Building internal tools requires exactly the same approach: early and frequent engagement with real users to ensure what you're building actually solves their problem. Otherwise, you might end up building and using tools that nobody uses. This is discouraging for those who work on the tools and frustrating for those who have to use tools they don't like. It also represents significant costs for the company in building a poor technology match for its users.  A team should use the following pattern in order to maximize their chances of successfully solving customer problems:   * Gather customer feedback first, before defining any potential features.  * Validate that you're solving a real problem.  * Iterate on a solution that actually solves that problem (and nothing more).  * Ensure the solution results in a viable business (for example, the cost is less than the anticipated revenue).  * Track key metrics to gauge success (for example, AARRR).  * Iterate through the above to improve those metrics. Success requires you to not only deploy and release software quickly, but to address customer needs better, smarter, and faster. This can be achieved by experimenting more thoroughly than your competition. Retail is one industry that has seen particular success with this  (PDF). Techniques such as hypothesis-driven development, defining and measuring your customer acquisition funnel, and A/B testing allow you to perform user experiments. This can foster creativity, speed up innovation, and create organizational learning.  Increased engagement with customers and participation in product management processes contributes to stronger identification with your organization's goals and values  (PDF). This in turn helps contribute to organizational performance.  ", "completion": "DevOps process: Customer feedback"}
{"prompt": "# Common pitfalls  Some common pitfalls to effectively using customer feedback include the following:   * Failing to gather feedback. It's common for software development companies to not gather customer feedback at all.  * Gathering feedback too late. Sometimes companies gather customer feedback so late in the software delivery lifecycle that they cannot act on it.  * Not understanding the problem (or misinterpreting customer feedback). Companies can have unrealistic expectations of their customers, or might not understand what customers want from the product. The team might even explicitly ignore inconvenient but accurate customer feedback. This situation can arise when a development team hasn't adequately evaluated or managed the risk of delivering the solution at hand, or simply doesn't understand the problem to solve. A development team wants to keep the scope of their work to the minimum that's required to solve a problem. In some cases, if the problem is larger than what the team has designed for, the team might incorrectly dismiss the additional work as 'scope creep.'  This results in an incomplete solution from the customer's perspective, and ultimately results in a failed product.  * Failing to allow teams to act on feedback. Delivery teams must be empowered to make changes to the design or specification of the system in response to feedback, as discussed in the document on team experimentation.  * Measuring success based on the wrong metric. In some cases, solutions without sufficient customer validation are provided to development and delivery teams as finalized requirements. In those cases, the success of the delivery team is then measured based on whether they delivered the feature as specified, not on whether the team actually solved the problem or achieved the outcome for customers.  * Failing to address the customer's problem. Industry data  (PDF) from A/B testing shows that in successful products, only about one-third of proposed features improve business outcomes when actually delivered. The remaining two-thirds of features deliver zero or negative outcomes for businesses. So, if you're not performing user research, odds are that at least two-thirds of the work that teams are doing is either not achieving the outcome they want, or it's actively making things worse. If a team is working on early-lifecycle products, particularly innovative ones, the team's odds at achieving the outcome they want are considerably worse.  * Underestimating the time and effort required to respond to customer feedback. Teams should expect that their proposed solutions are wrong and prepare to discard at least two-thirds of them. As for the remaining one-third of solutions, teams should be prepared to evolve them significantly in response to customer feedback. ", "completion": "DevOps process: Customer feedback"}
{"prompt": "# Ways to improve customer feedback  The field of user experience design (UX) provides a framework for understanding improvement. Many organizations treat UX as just making a product UI look good. However, UX is about whether you're solving a real problem for users; more broadly, UX is about every experience a user has when they interact with your organization. It cannot be overemphasized how critical good UX is to building successful products and services.  It's essential to build customer feedback gathering into the delivery process. Every significant feature you build should start by considering the problem to be solved. This should include performing user research to determine possible solutions and select a candidate. User research should be analyzed before a single line of code is written.  For early-lifecycle products, teams should adopt the ideas put forward in the lean startup movement  to validate the underlying business model of the product before any code is written. You should validate that you're solving a real problem, and then iterate on a solution that solves the problem while providing a viable business model.  You should follow a similar pattern for existing products that are implementing a new solution to a known business problem. When the solution design has been validated, you should create a prototype so that you can perform further research and testing. Only when testing validates that the feature achieves your goal should the full production feature be completed.  Many organizations skip all of this work and fail. However, even strictly following these steps does not guarantee success. The point of this effort is to do as much as possible to minimize the risk of failure.  ", "completion": "DevOps process: Customer feedback"}
{"prompt": "# Ways to measure customer feedback  Sophisticated data gathering isn't required to establish whether customer feedback is gathered, visible, and acted upon. The following questions can help you determine how well you're taking advantage of customer feedback for your product design:   * Do you have metrics that measure customer satisfaction? Are these updated and broadcast to teams regularly? Do you act on them?  * Do you validate all features before building them and perform user research with prototypes as part of delivery?  * Do you make changes to features based on this user research?  * Do you actively and regularly gather feedback from users and act on it? ", "completion": "DevOps process: Customer feedback"}
{"prompt": "# DevOps process: Team experimentation  Even in many so-called agile teams, developers can only work on requirements or stories that are given to them. And despite the developers’ specialist knowledge, or what they discover in the development process, they can’t change those requirements and stories. In truly agile teams, what’s written on the story card is a reminder of a conversation between customers and the team. Stories start from the business outcome that they are trying to achieve or the problem they are trying to solve. Teams then decide on what needs to be done, and test whether it will achieve the outcome or solve the problem.  For your organization to fully benefit from modern software development techniques, you must empower your teams to experiment with real users to achieve agreed-upon business outcomes. In this paradigm, developers can quickly prototype and test ideas as they discover more about users and about the problem, and design solutions. Teams then incorporate what they learned into the design of the product or service. Using the lean product management method, these practices help teams ship features that add value to the organization, and ship those features more frequently.  Team experimentation is part of lean product management. This approach is often used in combination with capabilities like visibility of work in the value stream, working in small batches, and visibility into customer feedback. These capabilities predict software delivery performance and organizational performance.  ", "completion": "DevOps process: Team experimentation"}
{"prompt": "# How to implement team experimentation  DevOps Research and Assessment (DORA) research  identifies three key components to team experimentation that drive software delivery performance:   * The ability to work on new ideas independently, without having to get permission from outside of the team.  * The ability to write and change specifications during development.  * The ability to make changes to stories and specifications without having to get permission from outside of the team. Based on these abilities, the following practices can help improve your team experimentation:   * Empower teams. Empower teams and allow them to work on new ideas in pursuit of business goals that solve important problems.  * Provide information and context. Providing teams with information and context lets teams make informed decisions about the right work to do. Measuring organizational outcomes provides information critical to making the best decisions, so teams are able to achieve expected outcomes and solve problems.  * Leave the details to those doing the work. Allow your teams to change stories, specifications, and technologies when they decide it's appropriate. Understand and acknowledge that they are the experts, and empower them to make the technical decisions necessary to get the work done. In the highest-performing teams and organizations, teams are allowed to make informed decisions about the tools and technologies they use. ", "completion": "DevOps process: Team experimentation"}
{"prompt": "# Common pitfalls in team experimentation  When organizations implement team experimentation, they often face these common pitfalls:   * Ignoring the importance of experimentation altogether. Don't treat your technical staff as order-takers. It's worse when teams are given specific direction on how to perform the work. Your technical staff are the experts, so empower them to make the technical decisions needed to get the work done.  * Not providing opportunities or time for your technical staff to design and conduct experiments in quick and easy ways. Empower your developers to quickly prototype and test ideas. ", "completion": "DevOps process: Team experimentation"}
{"prompt": "# Ways to improve team experimentation  To free your teams to find the best solutions, try some of these suggestions:   * Hold regular hackathons. Hackathons are opportunities for the team to experiment and to work with and share ideas. They also have the added benefit of letting your team work with new technologies and tools.  * Encourage teams to iterate on and continually improve solutions to foster experimentation. Many times the first solution to a problem isn't the best. Improvements to one service or feature often yield improvements in others.  * Allow developers and operators to talk to and observe customers. This kind of interaction provides more context and information that teams can use to solve problems and develop new ideas. ", "completion": "DevOps process: Team experimentation"}
{"prompt": "# Ways to measure team experimentation  You might be able to capture some evidence of team experimentation in your systems, for example, by tracking permission requests for changing stories or specifications. However, we recommend using surveys to measure experimentation. The core idea is that teams feel the freedom to do this work and find the best ways to solve problems.  DORA research shows that high-performance teams are driven by the following measures:   * Teams are able to work on new ideas.  * Teams are able to do work without having to ask for permission.  * Teams are able to make changes to stories and specifications during development.  * Teams are able to make changes without having to ask for permission. ", "completion": "DevOps process: Team experimentation"}
{"prompt": "# DevOps culture: Learning culture  Research from the DevOps Research and Assessment (DORA)  (PDF) team shows that an organizational culture that values learning contributes to software delivery performance with the following:   * Increased deployment frequency  * Reduced lead time for changes, time to restore service, and change failure rate  * Strong team culture The climate for learning in your organization is directly related to the extent to which your organization treats learning as strategic:   * Does your organization view learning as an investment necessary for growth?  * Is learning seen as a necessary burden, undertaken only grudgingly?  * Is learning completely avoided? Research done in other areas, such as accounting, has also shown that a climate for learning is predictive of performance gains.  ", "completion": "DevOps culture: Learning culture"}
{"prompt": "# DevOps process: Streamlining change approval  Most IT organizations have change management processes to manage the life cycle of changes to IT services, both internal and customer-facing. These processes are often the primary controls to reduce the operational and security risks of change.  Change management processes often include approvals by external reviewers or change approval boards (CABs) to promote changes through the system.  Compliance managers and security managers rely on change management processes to validate compliance requirements, which typically require evidence that all changes are appropriately authorized.  Research by DevOps Research and Assessment (DORA), presented in the 2019 State of DevOps Report (PDF), finds that change approvals are best implemented through peer review during the development process, supplemented by automation to detect, prevent, and correct bad changes early in the software delivery life cycle. Techniques such as continuous testing, continuous integration, and comprehensive monitoring and observability provide early and automated detection, visibility, and fast feedback.  Further, organizations can improve their performance by doing a better job of communicating the existing process and helping teams navigate it efficiently. When team members have a clear understanding of the change approval process, this drives higher performance.  ", "completion": "DevOps process: Streamlining change approval"}
{"prompt": "# How to implement a change approval process  Two important goals of the change approval process are decreasing the risk of making changes, and satisfying regulatory requirements. One common regulatory requirement is segregation of duties, which states that changes must be approved by someone other than the author, thus ensuring that no individual has end-to-end control over a process.  Traditionally, these goals have been met through a heavyweight process involving approval by people external to the team proposing the change: a change advisory board (CAB) or a senior manager. However, DORA's research shows that these approaches have a negative impact on software delivery performance. Further, no evidence was found to support the hypothesis that a more formal, external review process was associated with lower change fail rates.  Such heavyweight approaches tend to slow down the delivery process leading to the release of larger batches less frequently, with an accompanying higher impact on the production system that is likely to be associated with higher levels of risk and thus higher change fail rates. DORA's research found this hypothesis was supported in the data.  Instead, teams should:   * Use peer review to meet the goal of segregation of duties, with reviews, comments, and approvals captured in the team's development platform as part of the development process.  * Employ continuous testing, continuous integration, and comprehensive monitoring and observability to rapidly detect, prevent, and correct bad changes.  * Treat your development platform as a product that makes it easy for developers to get fast feedback on the impact of their changes on multiple axes, including security, performance, and stability, as well as defects. Your goal should be to make your regular change management process fast and reliable enough that you can use it to make emergency changes too.  In the continuous delivery paradigm the CAB still has a vital role, which includes:   * Facilitating notification and coordination between teams.  * Helping teams with process improvement work to increase their software delivery performance.  * Weighing in on important business decisions that require a trade-off and sign-off at higher levels of the business, such as the decision between time-to-market and business risk. This new role for the CAB is strategic. By shifting detailed code review to practitioners and automated methods, the time and attention of those in leadership and management positions is freed up to focus on more strategic work. This transition, from gatekeeper to process architect and information beacon, is consistent with the practices of organizations that excel at software delivery performance.  ", "completion": "DevOps process: Streamlining change approval"}
{"prompt": "# Common pitfalls in change approval processes  The following pitfalls are common to change approval processes:  Reliance on a centralized Change Approval Board (CAB) to catch errors and approve changes. This approach can introduce delay and often error. CABs are good at broadcasting change, but people that far removed from the change might not understand the implications of those changes.  Treating all changes equally. When all changes are subject to the same approval process, change review is inefficient, and people are unable to devote time and attention to those that require true concentration because of differences in risk profile or timing.  Failing to apply continuous improvement. As with all processes, key performance metrics such as lead time and change fail rate should be targeted with the goal of improving the performance of the change management process, including providing teams with tools and training to help them navigate it more effectively.  Responding to problems by adding more process. Often organizations use additional process and more heavyweight approvals when faced with stability problems in production. Analysis suggests this approach will make things worse because this drives up lead times and batch sizes, creating a vicious cycle. Instead, invest in making it quicker and safer to make changes.  ", "completion": "DevOps process: Streamlining change approval"}
{"prompt": "# Ways to improve your change approval process  To improve your change approval processes, focus on implementing the following:   * Moving to a peer-review based process for individual changes, enforced at code check-in time, and supported by automated tests.  * Finding ways to discover problems such as regressions, performance problems, and security issues in an automated fashion as soon as possible after changes are committed.  * Performing ongoing analysis to detect and flag high risk changes early on so that they can be subjected to additional scrutiny.  * Looking at the change process end-to-end, identifying bottlenecks, and experimenting with ways to shift validations into the development platform.  * Implementing information security controls at the platform and infrastructure layer and in the development tool chain, rather than reviewing them manually as part of the software delivery process. Research from the 2019 State of DevOps Report (PDF)  shows that while moving away from traditional, formal change management processes is the ultimate goal, simply doing a better job of communicating the existing process and helping teams navigate it efficiently has a positive impact on software delivery performance. When team members have a clear understanding of the process to get changes approved for implementation, this drives high performance. This means they are confident that they can get changes through the approval process in a timely manner and know the steps it takes to go from 'submitted' to 'accepted' every time for all the types of changes they typically make.  ", "completion": "DevOps process: Streamlining change approval"}
{"prompt": "# Ways to measure change approval in your systems  Now your teams can list possible ways to measure change approval:  While you consider your own environment, you will likely develop your own measures to understand and gain insight into your change approval processes. We suggest you use these to not only measure your process but also work to improve it.  ", "completion": "DevOps process: Streamlining change approval"}
{"prompt": "# How to implement a learning culture  You can help your organization create a climate for learning by viewing learning as the key to improvement and as an investment. Some steps you can take  to directly support learning include:   * Create a training budget, and advocate for it internally. Emphasize how much the organization values a climate of learning by putting resources behind formal education opportunities.  * Ensure that your team has the resources to engage in informal learning and the space to explore ideas. Learning often happens outside of formal education. Some companies, like 3M and Google, set aside a portion of time for focused free-thinking and exploration of side projects.  * Make it safe to fail. If failure is punished, people won't try new things. Treat failures as opportunities to learn, and hold blameless post-mortems to work out how to improve processes and systems. Help people feel comfortable taking reasonable risks, and create a culture of innovation.  * Create opportunities and spaces to share information. Whether you hold weekly lightning talks or offer resources for monthly lunch-and-learns, set up a regular cadence of opportunities for employees to share their knowledge.  * Make resources available for continued education. For example, attending conferences is important for both exposure to new technology and case studies, as well as networking with peers. ", "completion": "DevOps culture: Learning culture"}
{"prompt": "# Ways to improve your learning culture  Continuing to build a climate for learning is directly related to how an organization encourages and invests in learning. Here are some ways an organization can show that learning is important and necessary for growth:   * Have regular lunchtime meetings ('brownbags') where one person presents about a project they are working on in a new tech, or something they are investigating. Rotate the presentations among team members and reward people for presenting.  * When people attend conferences, have them share the new ideas through presentations or trip reports. You can even host regular meetups or mini-conferences to increase networking and exposure to new technologies and ideas.  * Encourage people to get certifications or external trainings. You can help with this by covering costs of external trainings and forming study groups that are a part of normal work activity. ", "completion": "DevOps culture: Learning culture"}
{"prompt": "# Ways to measure learning culture  The most effective way to measure your climate for learning is to survey your employees. Use well-designed items and constructs that are rigorous and show good psychometric properties  of discriminant  and convergent  validity and reliability (all statistical measures). The goal is for teams to feel that learning is encouraged and supported throughout the organization.  Research presented in the article An empirical analysis of the levers of control framework  shows that you can measure learning culture based on the extent to which people agree or disagree with the following statements about their organization:   * Learning is the key to improvement.  * Once we quit learning we endanger our future.  * Learning is viewed as an investment, not an expense. Research from DORA shows that a climate for learning is a significant predictor of software delivery performance as well as organizational performance. Other studies show that climate for learning is predictive of organizational performance, so investments in learning, and a culture that values learning, is a good bet for organizations.  ", "completion": "DevOps culture: Learning culture"}
{"prompt": "# DevOps culture: Westrum organizational culture  According to research by DevOps Research and Assessment (DORA), organizational culture that is high-trust and emphasizes information flow is predictive of software delivery performance and organizational performance in technology. The idea that a good culture that optimizes information flow is predictive of good outcomes is not a new idea; it is based on research by sociologist Dr. Ron Westrum. Westrum's research  included human factors in system safety, particularly in the context of accidents in technological domains such as aviation and healthcare.  In his work with these high risk, highly complex fields, Westrum noted that such a culture influences the way information flows through an organization. Westrum provides three characteristics of good information:   * It provides answers to the questions that the receiver needs answered.  * It is timely.  * It is presented in such a way that the receiver can use it effectively. In his research, he developed the following typology of organizational cultures.  Table 1: The Westrum organizational typology model: How organizations process information (Source: Ron Westrum, 'A typology of organisation culture', BMJ Quality & Safety 13, no. 2 (2004), doi:10.1136/qshc.2003.009522.)  Similar to Dr. Westrum's findings, DORA research shows that a high-trust, generative culture predicts software delivery and organizational performance in technology.  Research from a large two-year study at Google found similar results: that high performing teams need a culture of trust and psychological safety, meaningful work, and clarity. In the 2019 State of DevOps Report further analysis shows that a culture of psychological safety is predictive of software delivery performance, organizational performance, and productivity.  ", "completion": "DevOps culture: Westrum organizational culture"}
{"prompt": "# How to implement organizational culture  DORA research shows that changing the way people work changes culture; this is echoed in the work of John Shook, who spoke of his experiences in transforming culture: 'The way to change culture is not to first change how people think, but instead to start by changing how people behave—what they do.' This is seen in the technical, lean, and product management capabilities they have researched that influence culture.  In addition, teams can identify helpful practices to create a generative culture that fosters information flow and trust by examining the six aspects of Westrum's model of organizational culture, focusing on those behaviors seen in the generative culture:   * High cooperation.  * Messengers are trained.  * Risks are shared.  * Bridging is encouraged.  * Failure leads to inquiry.  * Novelty is implemented. Based on these aspects, here are some practices you can implement to improve your culture:   * High cooperation. Create cross-functional teams that include representatives from each functional area of the software delivery process (business analysts, developers, quality engineers, ops, security, and so on). This practice lets everyone share the responsibility for building, deploying, and maintaining a product. It's also important that there is good cooperation within the team.  * Train the messengers. This means we want people to bring us bad news so we can make things better. Hold blameless postmortems. By removing blame, you remove fear; and by removing fear, you enable teams to surface problems and solve them more effectively. Also create and foster an environment where it is safe to take smart risks and fail, so that anyone can surface problems at any time—even without the ceremony of a postmortem.  * Share risks. Along with this, encourage shared responsibilities. Quality, availability, reliability and security are everyone's job. One way to improve the quality of your services is to ensure that developers share responsibility for maintaining their code in production. The improvement in collaboration that comes from sharing responsibility inherently reduces risk: The more eyes on the software delivery process, the more you'll avoid errors in process or planning. Automation also reduces risk, and with the right tool choice, can enable collaboration.  * Encourage bridging. Break down silos. In addition to creating cross-functional teams, techniques for breaking down silos include co-locating ops with the dev team; including ops in planning throughout the software delivery lifecycle; and implementing ChatOps. Another tip is to identify someone in the organization whose work you don't understand (or whose work frustrates you, like procurement) and invite them to coffee or lunch. Informal discussions help foster better communication, and you may understand why they do what they do—and you can come up with creative solutions together.  * Let failure lead to inquiry. Again, hold blameless postmortems. The response to failure shapes the culture of an organization. Blaming individuals for failures creates a negative culture. If instead, failures lead you to ask questions about what caused the failures and how you can keep them from happening again in the future, you've improved your technical system, your processes, and your culture.  * Implement novelty. Encourage experimentation. Giving employees freedom to explore new ideas can lead to great outcomes. Some companies give engineers time each week for experimentation. Others host internal hack days or mini-conferences to share ideas and collaborate. Many new features and products began this way. When you release your employees from habitual pathways and repetitive tasks, they can generate enormous value for your organization. And remember that novelty isn't limited to new products and features. Also encourage and reward improvements in process and ideas that help foster collaboration. ", "completion": "DevOps culture: Westrum organizational culture"}
{"prompt": "# Common pitfalls of organizational culture  Organizational cultures with high information flow often encounter the following pitfalls:   * Ignoring the importance of culture altogether, and treating technical work as a technology and process challenge only.  * Focusing only on local team cultures, and not reaching out to also understand how team cultures interact with the broader organizational culture.  * Not having enough support, or appropriate support, from leaders and managers for the necessary culture shift and transformation.  * Ignoring or punishing bad news. It's better to understand that failures are inevitable in complex systems, and treat them as opportunities to improve and learn.  * Not encouraging novelty. Admiral Grace Hopper, the famed computer science pioneer and inventor of the compiler, once said, 'The most dangerous phrase in the language is ‘we've always done it this way.'' Allowing and encouraging teams to experiment and try new things will make work better. By focusing on the six aspects of Westrum's typology, teams and organizations can work thoughtfully and meaningfully toward improving their culture.  ", "completion": "DevOps culture: Westrum organizational culture"}
{"prompt": "# How to measure organizational culture  Organizational culture is a perceptual measure, and therefore, best measured using survey methods. The Westrum survey measures, included here, are highly valid and reliable statistically.   * On my team, information is actively sought.  * Messengers are not punished when they deliver news of failures or other bad news.  * On my team, responsibilities are shared.  * On my team, cross-functional collaboration is encouraged and rewarded.  * On my team, failure causes inquiry.  * On my team, new ideas are welcomed. Present these measures together—unlabeled and untitled—with responses ranging from Strongly Disagree (=1) to Neither Agree nor Disagree (=4) to Strongly Agree (=7). They are a latent construct, which means you can average their scores to provide a single score for your Westrum culture metric. If necessary, you can alter the items slightly to fit your context, but we recommend doing only minor changes in order to preserve the statistical properties.  ", "completion": "DevOps culture: Westrum organizational culture"}
{"prompt": "# DevOps culture: Job satisfaction  Early analysis performed by DevOps Research and Assessment (DORA)  found that job satisfaction is a predictor of organizational performance. Having engaged employees doing meaningful work drives business value.  Everybody knows how job satisfaction feels. It's about doing work that's challenging and meaningful, and being empowered to exercise skills and judgment. Where there's job satisfaction, employees bring the best of themselves to work: their engagement, their creativity, and their strongest thinking. The result is more innovation in any area of the business, including technology.  There's a virtuous circle when it comes to the benefits of job satisfaction. People do better work when they feel supported by their employers, when they have the tools and resources to do their work, and when they feel their judgment is valued. Better work results in higher software delivery performance, which results in higher organizational performance.  This cycle of continuous improvement and learning is what sets successful companies apart, enabling them to innovate, get ahead of the competition, and win.  ", "completion": "DevOps culture: Job satisfaction"}
{"prompt": "# Common pitfalls in job satisfaction  The following pitfalls are commonly related to job satisfaction:   * Not giving people the tools  they need to be successful.  * Not giving people meaningful work. Practitioners and leaders must remember that technology transformations are hard and take time, and often require an update in both technology and skill sets. Technology transformations also commonly require organizational changes like reorganizations and culture shifts, which can be difficult for people to navigate. If you're trying to institute change, don't forget that you must make time and resources available for improvement work. Creating change takes time, and people also need time to adjust to the changes, as you build practices such as automation  and continuous integration  into your delivery process. On top of that, improving process is itself a skill that needs to be learned. Teams that routinely work on improvement get better at it over time, and are more likely to stay with the company.  ", "completion": "DevOps culture: Job satisfaction"}
{"prompt": "# Ways to improve job satisfaction  DORA research on job satisfaction recommends the following key actions:   * Give employees the tools and resources needed to do their work. Employees must have the tools necessary to get their work done, and teams that can decide which tools they use  do better at continuous delivery. Teams that can choose their own tools make these choices based on how they work, and on the tasks they need to perform. No one knows better than practitioners what they need to be effective, so it's not surprising that practitioner tool choice helps to drive better outcomes. Employees must also have the resources necessary to do their work. Those might be technical resources, such as access to servers or environments necessary to develop and test, or resources needed to learn and develop new skills, such as access to course materials and budget to attend trainings or technical conferences. Give employees the tools and resources needed to do their work.  Employees must have the tools necessary to get their work done, and teams that can decide which tools they use  do better at continuous delivery. Teams that can choose their own tools make these choices based on how they work, and on the tasks they need to perform. No one knows better than practitioners what they need to be effective, so it's not surprising that practitioner tool choice helps to drive better outcomes.  Employees must also have the resources necessary to do their work. Those might be technical resources, such as access to servers or environments necessary to develop and test, or resources needed to learn and develop new skills, such as access to course materials and budget to attend trainings or technical conferences.   * Give employees meaningful work that leverages their expertise. The importance of meaningful work can't be overstated. In some studies, employees have rated the importance of meaningful work just as highly as the importance of salary. Meaningful work makes a difference and is often very personal. Give employees meaningful work that leverages their expertise.  The importance of meaningful work can't be overstated. In some studies, employees have rated the importance of meaningful work just as highly as the importance of salary. Meaningful work makes a difference and is often very personal.  ", "completion": "DevOps culture: Job satisfaction"}
{"prompt": "# Ways to measure job satisfaction  Measuring job satisfaction in systems is hard. There just isn't a good way to proxy job satisfaction in system data. Much like organizational culture, job satisfaction is a perceptual measure, so to measure it, you must ask people for their opinions. If you worry that you won't get accurate answers, that's a signal that something is wrong and it's worth looking into.  ", "completion": "DevOps culture: Job satisfaction"}
{"prompt": "# DevOps culture: Transformational leadership  DevOps Research and Assessment (DORA) research shows that effective leadership has a measurable, significant impact on software delivery outcomes. However, rather than driving these outcomes directly, effective transformational leaders influence software delivery performance by enabling the adoption of technical and product management capabilities and practices by practitioners, which in turn drives the outcomes leaders care about.  To study the role of leadership in DevOps transformations, DORA used a measure of transformational leadership that includes five dimensions. According to this model, validated in Dimensions of transformational leadership: Conceptual and empirical extensions  (Rafferty, A. E., & Griffin, M. A.), the five characteristics of a transformational leader are the following:   * Vision: Understands clearly where their team and the organization are going, and where they want the team to be in five years.  * Inspirational communication: Says positive things about the team; says things that make employees proud to be a part of their organization; encourages people to see changing conditions as situations full of opportunities.  * Intellectual stimulation: Challenges team members to think about old problems in new ways and to rethink some of their basic assumptions about their work; has ideas that force team members to rethink some things that they have never questioned before.  * Supportive leadership: Considers others' personal feelings before acting; behaves in a manner which is thoughtful of others' personal needs; sees that the interests of team members are given due consideration.  * Personal recognition: Commends team members when they do a better than average job; acknowledges improvement in quality of team members' work; personally compliments team members when they do outstanding work. These five characteristics of transformational leadership are highly correlated with software delivery performance. In fact, DORA observed statistically significant differences in leadership characteristics between high-, medium- and low- performing software delivery teams (see the 2017 State of DevOps Report  pp12-19). High-performing teams reported having leaders with the strongest behaviors across all dimensions. In contrast, low-performing teams reported the lowest levels of these leadership characteristics.  What was most striking, however, was that teams with the least transformative leaders (the bottom third) were also far less likely to be high performers at software delivery — in fact, they were half as likely to exhibit high software delivery performance. This validates common experience: Though there are many DevOps and technology transformation success stories emerging from the grassroots, it is far easier to achieve success when you have effective, transformational leadership.  Moving beyond correlation to look at how effective transformational leaders achieve results, the results are interesting. The DORA team created a predictive model using a technique called structural equation modeling  to test the relationships between transformational leadership, a number of technical and product management practices, and software delivery and organizational performance. The validated model is shown in the following diagram. You can read the arrows as driving or impacting the capabilities and outcomes to which they point.    The validated model shows that effective leaders impact software delivery and organizational performance indirectly, by enabling teams to adopt technical practices and lean product management practices. It is these practices that drive organizational outcomes such as higher software delivery performance and organizational performance. These capabilities also drive cultural change, as shown in the overall research program.  ", "completion": "DevOps culture: Transformational leadership"}
{"prompt": "# How to implement transformational leadership  Transformational leadership can be contrasted with transactional leadership, where employees are rewarded with money or prestige for complying with leadership instructions or punished for failing to follow them.  However, transformational leaders, according to Rafferty and Griffin, 'motivate followers to achieve performance beyond expectations by transforming followers' attitudes, beliefs, and values as opposed to simply gaining compliance.'  DORA's research found evidence that the presence of leaders with transformational characteristics is not enough to achieve high performance. Looking at teams whose leaders were in the top 10% in terms of transformational leadership, the research found that they were not the very highest performers. In fact, these teams displayed significant variation in levels of software delivery performance.  This result can be explained by the observation that leaders cannot achieve higher performance on their own. Success also depends on the implementation of effective technical, management, and product management practices, along with the other capabilities discussed in DORA's research. It's essential that the transformational leadership behaviors, described above, are directed towards the implementation of these capabilities.  Take vision as an example. One way to create a clear vision for a software delivery team is to set measurable relative goals for software delivery performance. For example  Richard Herbert, CIO for Global Banking at Markets of HSBC, set every team the goal to 'double the frequency of releases, half the number of low impact incidents, and quarter the number of high impact incidents.'  There may be significant obstacles to achieving goals like the ones set by Richard Herbert. Again, leaders can use intellectual stimulation to help teams identify and remove obstacles to achieving higher performance. Perhaps team members believe that implementing continuous testing will help them, but they've tried before and failed. Leaders can ask teams questions such as: 'Why did it fail last time?', 'What lessons did you learn?', 'What would you do differently this time?', 'What ideas would you like to try this time?'.  Personal recognition is also important, and must be directed such that it reinforces behaviors that help teams improve. Examples include trying experiments even if they don't work, or taking time to help other teams implement new ideas. Another example of effective personal recognition is e-commerce company Etsy, which at its annual engineering conference gives an award  'to the engineer who inadvertently causes the most interesting or most learning-filled incident.'  It's crucial that these behaviors are demonstrated consistently, and particularly when the team is under stress.  Finally, remember that leadership doesn't just mean executives and managers: anybody can be a leader. Almost all of these behaviors can be practiced by everybody in an organization. Consider how you can build them into your daily interactions with other people in your organization.  ", "completion": "DevOps culture: Transformational leadership"}
{"prompt": "# How to measure transformational leadership  Transformational leadership can be measured directly by asking team members about the extent to which they believe leaders exhibit the behaviors described.  The effects of transformational leadership are also measurable. For example, if a leader does an outstanding job of defining and communicating their vision, everybody in the organization should be able to describe that vision in a consistent way without having to look it up.  ", "completion": "DevOps culture: Transformational leadership"}
